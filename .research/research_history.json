{
  "research_topic": "Proposing an improved Chain-of-Thought based on human thinking methods, evaluated purely through prompt tuning without fine-tuning or time-intensive experiments",
  "queries": [
    "Chain-of-Thought human prompt"
  ],
  "research_study_list": [
    {
      "title": "Chain-of-Thought Reasoning Without Prompting",
      "full_text": "arXiv:2305.10978v7  [cs.LG]  2 Jun 2025 Client Selection for Federated Policy Optimization with Environment Heterogeneity Zhijie Xie zhijie.xie@connect.ust.hk Shenghui Song eeshsong@ust.hk Department of Electronic and Computer Engineering The Hong Kong University of Science and Technology Clear Water Bay, Kowloon, Hong Kong Abstract The development of Policy Iteration (PI) has inspired many recent algorithms for Reinforce- ment Learning (RL), including several policy gradient methods that gained both theoretical soundness and empirical success on a variety of tasks. The theory of PI is rich in the context of centralized learning, but its study under the federated setting is still in the infant stage. This paper investigates the federated version of Approximate PI (API) and derives its error bound, taking into account the approximation error introduced by environment heterogene- ity. We theoretically prove that a proper client selection scheme can reduce this error bound. Based on the theoretical result, we propose a client selection algorithm to alleviate the additional approximation error caused by environment heterogeneity. Experiment results show that the proposed algorithm outperforms other biased and unbiased client selection methods on the federated mountain car problem, the Mujoco Hopper problem, and the SUMO-based autonomous vehicle training problem by effectively selecting clients with a lower level of heterogeneity from the population distribution. Keywords: Federated Reinforcement Learning, Client Selection, Data Heterogeneity, Policy Iteration, Communication Efficiency 1 Introduction Reinforcement Learning (RL) has been applied to many real-world applications ranging from gaming and robotics to recommender systems (Silver et al., 2016; Chen et al., 2019). However, single-agent RL often suffers from poor sample efficiency, resulting in slow convergence and a high cost of sample collection (Ciosek and Whiteson, 2020; Fan et al., 2021; Papini et al., 2018). Therefore, it is desirable to deploy RL algorithms to large-scale and distributed systems where multiple agents can contribute to the learning collaboratively. However, Multi-Agent RL (MARL) (Zhang et al., 2019) and parallel RL (Nair et al., 2015; Mnih et al., 2016) require intensive communication among agents or data sharing, which may not be practical due to both the communication bottleneck and privacy concerns of many real-world applications. For example, privacy is a major concern in autonomous driving (Liang et al., 2019; Li et al., 2022), and sharing data among vehicles is not allowed. To this end, Federated Learning (FL) (Xianjia et al., 2021; Na et al., 2023; Lim et al., 2020), which enables multiple clients to jointly train a global model without violating user privacy, is an appealing solution for addressing the sample inefficiency and privacy issue of RL in innovative applications such as autonomous driving, IoT network, and healthcare (Zhou et al., 2022). As a result, ©2025 Zhijie Xie and Shenghui Song. License: CC-BY 4.0, seehttps://creativecommons.org/licenses/by/4.0/.Xie and Song Federated Reinforcement Learning (FRL) has attracted much research attention (Qi et al., 2021). Despite the significant progress of empirical works on FRL (Qi et al., 2021), the commu- nity’s understanding of FRL is still in its infancy, especially from the theoretical perspective. For example, the sample efficiency of Policy Gradient (PG) methods is typically low due to the large variance in gradient estimation. This issue could be exacerbated in the con- text of FL, where clients with heterogeneous environments can generate a diverse range of trajectories. To address this problem, a variance-reduced policy gradient method, namely Federated Policy Gradient with Byzantine Resilience (FedPG-BR), was proposed together with an analysis of the sample efficiency and convergence guarantee (Fan et al., 2021). While clients are assumed to be homogeneous in FedPG-BR, another line of work, termed FedKL (Xie and Song, 2023), noticed that the environment heterogeneity imposes an extra layer of difficulty in learning and proved that a Kullback-Leibler (KL) penalized local objective can generate a monotonically improving sequence of policies to accelerate convergence. The authors of QAvg & PAvg (Jin et al., 2022) provided a convergence proof for the federated Q-Learning and federated PG. QAvg offered important insights regarding how the Bellman operators can be generalized to the federated setting and proposed a useful tool, i.e., the imaginary environment (the average of all clients’ environments), for analyzing FRL. More recently, FedSARSA (Zhang et al., 2024) studied the integration of FRL and SARSA, where SARSA is an on-policy Temporal Difference (TD) algorithm. However, there has not been any convergence analysis regarding Policy Iteration (PI) in FRL in the literature. Given PI’s application and theoretical importance, it is desirable to fill this knowledge gap and derive efficient FRL algorithms accordingly. Among existing RL methods, PI is one of the most popular ones and serves as the foundation of many policy optimization methods, e.g., Safe Policy Iteration (SPI) (Pirotta et al., 2013), Trust Region Policy Optimisation (TRPO) (Schulman et al., 2015), and Deep Conservative Policy Iteration (DCPI) (Vieillard et al., 2020). With exact PI, convergence to the optimal policy is guaranteed under mild conditions. However, exact policy evaluation and policy improvement are normally impractical. With Approximate Policy Iteration (API) (Bertsekas, 2022; Bertsekas and Tsitsiklis, 1996), it is assumed that the approximation error is inevitable, and only estimates of the value function and improved policy with bounded errors are available. In the presence of these approximation errors, convergence is not ensured, but the difference in value functions between the generated policy and the optimal policy is bounded (Bertsekas, 2022). In some cases, the algorithm ends up generating a cycle of policies, which is called the policy oscillation/chattering phenomenon (Bertsekas, 2011; Wagner, 2011). Unfortunately, FRL with heterogeneous environments will introduce extra approximation errors into the policy iteration process, making the associated analysis more challenging. As will be shown in the following sections, this error is proportional to the level of heterogeneity of the system, and client selection is an effective way to alleviate this problem. There exist various client selection schemes for Federated Supervised Learning (FSL) and most of them can be classified into two categories: (1) unbiased client selection; and (2) biased client selection. Convergence guarantee for both schemes has been studied and generalized to tackle the heterogeneity issue of FSL (Li et al., 2020; Li et al., 2020; Jee Cho 2Client Selection for Federated Policy Optimization et al., 2022). However, to the best of the authors’ knowledge, there is no known client selection scheme specifically designed to tackle the heterogeneity issue of FRL. Contributions. In this paper, we derive the error bound of Federated Approximate Policy Iteration (FAPI) under heterogeneous environments, which is not yet available in the literature. The derived error bound takes the heterogeneity level of clients into consideration and explicitly reveals its impact. Based on the error bound, we propose a client selection algorithm to improve the convergence speed of federated policy optimization. The efficacy of the proposed algorithm is validated on the federated mountain car problem, the Mujoco Hopper problems, and the SUMO-based autonomous vehicle training problem. 2 Background In Section 2.1, we introduce the optimization problem of FRL. In Section 2.2, we review some known results on API. An imaginary environment is introduced in Section 2.3 to assist the following analysis. 2.1 Federated Reinforcement Learning The system setup of FRL in this paper is similar to that of FL (McMahan et al., 2017), i.e., a federated system consisting of one central server andN distributed clients. In the t-th training round, the central server broadcasts the current global policyπt to K selected clients which will performI iterations of local training. In each iteration, then-th client interacts with its environment to collectE trajectories and utilize them to update its local policy toπt+1 n . At the end of each round, the training results will be uploaded to the central server for aggregation to obtain the new global policyπt+1. We model the local learning problem of each client as a finite-state infinite-horizon discounted Markov Decision Process (MDP). Accordingly, the FRL system consists ofN finite MDPs {(S, µ,A, Pn, R, γ)|n ∈ {1, ..., N}}, where S denotes a finite set of states,µ represents the initial state distribution,A is a finite set of actions, andγ ∈ (0, 1) is the discount factor. The transition function Pn(s′|s, a) : S × S × A →[0, 1] represents the probability that the n-th MDP transits from states to s′ after taking actiona (Sutton and Barto, 2018). The reward functionR(s, a) : S × A →[0, Rmax] gives the expected reward for taking actiona in states, and we assume rewards are bounded and non-negative. As a result, then-th MDPMn can be represented by a 6-tuple(S, µ,A, Pn, R, γ) sharing the same state space, action space, initial state distribution, and reward function with other clients, but with possibly different transition probabilities. A client’s behavior is controlled by a stochastic policyπn : S × A →[0, 1] which outputs the probability of taking an action a in a given states. Throughout this work, we consider parameterized policiesπθ where πθ(a|s) is a differentiable function of the parameter vectorθ. For parameterized policies with t-indexed notation, we omit the parameter vector for notation simplicity and writeπt and πt n for the global policyπθt and then-th local policyπθt n, respectively. Furthermore, we define the state-value function V π n (s) = Eπ,Pn \" ∞X l=0 γlR(st+l, at+l)|st = s # , 3Xie and Song where the expectation is performed over actions sampled from policyπ and states sampled from the transition probabilityPn. It gives the expected return when the client starts from state s and follows policyπ thereafter in then-th MDP. For parameterized value functions with t-indexed notation, we omit the parameter vectorw for notation simplicity and write V t and V t n for V wt and V wt n, respectively. In each round, every client aims to train a local policy to maximize its expected discounted reward ηn(π) = Es0∼µ,at∼π,st+1∼Pn \" ∞X t=0 γtR(st, at) # , (1) or equivalently,ηn(π) = Es0∼µ [V π n (s0)]. The notationEs0∼µn,at∼π,st+1∼Pn indicates that the reward is averaged over all states and actions according to the initial state distribution, the transition probability, and the policy. Accordingly, the optimization problem for FRL can be formulated as max π η(π) where η(π) = NX n=1 qnηn(π), (2) where qn is the weight of then-th client. Denote the average value function of policyπ as ¯V π(s) = NX n=1 qnV π n (s), ∀s ∈ S, then we can rewrite (2) as max π η(π) where η(π) = Es0∼µ \u0002¯V π(s0) \u0003 . (3) The above formulation covers both heterogeneous and homogeneous cases. In particular, the different MDPs, i.e., different transition probabilities, represent the heterogeneous environments experienced by clients. All MDPs will be identical for the homogeneous case (Fan et al., 2021). It is worth noting that the optimization problem in (3) is often referred to as the Weighted Value Problem (WVP) in the latent MDP literature. Finding the optimal solution of WVP is NP-hard (Steimle et al., 2021). In contrast, an error bound showing the distance between the obtained policy and the optimal policy is feasible as demonstrated in Section 3. 2.2 Approximate Policy Iteration Given any MDPMn defined in Section 2.1, it is well known (Sutton and Barto, 2018) that the value functionV π n is the unique fixed point of the Bellman operatorTπ n : R|S| → R|S|, i.e., ∀s ∈ S, V∈ R|S| Tπ n V (s) = X a π(a|s)   R(s, a) + γ X s′ Pn(s′|s, a)V (s′) ! , V π n (s) = Tπ n V π n (s), 4Client Selection for Federated Policy Optimization where |S| denotes the cardinality ofS. Similarly, the optimal value functionV ∗ n is the unique fixed point of the Bellman operatorTn : R|S| → R|S|, i.e.,∀s ∈ S, V∈ R|S| TnV (s) = arg max a   R(s, a) + γ X s′ Pn(s′|s, a)V (s′) ! , V ∗ n (s) = TnV ∗ n (s). Note that the subscriptn of the Bellman operators denotes the index of transition probability with which the operator is applied. Both operators are monotonic and sup-norm contractive (Bertsekas and Tsitsiklis, 1996). Now we describe the classic API, which is an iterative algorithm that generates a sequence of policies and the associated value functions. Let∥·∥ denote the sup-norm, i.e. ∥V ∥ = sups∈S |V (s)|, ∀V ∈ R|S|, ∥·∥2 denote the l2-norm, i.e.∥V ∥2 = pP s∈S V (s)2, ∀V ∈ R|S|, and V ∗ denote the value function of the optimal policyπ∗. Given an initial policyπt, each iteration consists of two phases, whereδ and ϵ are some scalars: Policy Evaluation.The value functionV πt of the current policy is approximated byV t satisfying \r\r\rV t − V πt \r\r\r ≤ δ, t= 0, 1, ··· . (4) Policy Improvement.A greedy improvement is made to the policy with an approximation error \r\r\rTπt+1 V t − T Vt \r\r\r ≤ ϵ, t= 0, 1, ··· . (5) The following proposition gives the error bound of API. Proposition 1 The sequence \u0000 πt\u0001∞ t=0 generated by the API algorithm described by (4), (5) satisfies lim sup t→∞ \r\r\rV πt − V ∗ \r\r\r ≤ ϵ + 2γδ (1 − γ)2 , (6) The detailed proof of Proposition 1 can be found in Proposition 2.4.3 of Bertsekas (2022). 2.3 Imaginary MDP We define the imaginary MDP as in QAvg (Jin et al., 2022). Specifically, it is a MDP represented by the 6-tuple(S, µ,A, ¯P ,R, γ) where ¯P(s′|s, a) = NX n=1 qnPn(s′|s, a), ∀s′, s∈ S, a∈ A, denotes the average transition probability. Accordingly, we denote the Bellman operators in the imaginary MDP asTπ I and TI, where the subscriptI indicates that the transition probability is ¯P(s′|s, a), ∀s, a, s′. Moreover, denotingπ∗ I the optimal policy in the imaginary MDP MI, we have the value function ofπ and the optimal value function in the imaginary MDP V π I (s) = Tπ I V π I (s), V π∗ I I (s) = V ∗ I (s) = TIV ∗ I (s), respectively. The imaginary MDP is a handy tool to analyze the behavior of FAPI since it provides a unified view of all clients in the context of MDP where the theory of API is richly supplied with operator theory and the fixed point theorem. 5Xie and Song 3 Error Bound of FAPI In this section, we establish the error bound of FAPI under the framework of API and the imaginary MDP. The analysis shows that the FAPI process can be considered as learning in the imaginary MDP which eases the analysis. We focus our discussion on policy with function approximation where the error term introduced by the aggregation step is nontrivial. To establish FAPI’s error bound, we first examine the approximation error of policy evaluation and improvement within the FRL framework. Then, we can derive the distance between the optimal value function and the value function produced by FAPI. Throughout this work, we assume the policy iteration performed by each client is approximate. In particular, letV t n denote the evaluated value function of then-th client in round t, we have \r\r\rV t n − V πt n \r\r\r ≤ δn, max V ∈R|S| \r\r\rTπt+1 n n V − TnV \r\r\r ≤ ϵn, t = 0, 1, ··· . (7) In the following, we define a metric to quantify the level of heterogeneity of the FRL system. Specifically, we provide two metrics, which will be used for bounding the error of policy evaluation and policy improvement, respectively. Definition 2 (Level of heterogeneity). We define two parameters to measure the level of heterogeneity as κ1 = NX n=1 qnκn,I, κ 2 = X i,j qiqjκi,j, where Pπ n (s′|s) = P a π(a|s)Pn(s′|s, a), ∀s, s′ ∈ S, κi,j = maxπ,s P s′ \f\f\fPπ i (s′|s) − Pπ j (s′|s) \f\f\f and κn,I = maxπ,s P s′ \f\f\fPπ n (s′|s) − PN j=1 qjPπ j (s′|s) \f\f\f. Here, κ1 measures the average deviation of clients’ MDP from the imaginary MDP, and κ2 measures the average distance between each pair of clients in terms of the transition probability. With homogeneous environments, bothκ1 and κ2 will be equal to0. As the transition probability of each MDP gets farther away from each other,κ1 and κ2 tend to be larger and indicate a heterogeneous network. It is trivial to show thatκ1 ≤ κ2. 3.1 Federated Policy Evaluation The following lemmas describe the relation between the averaged value function¯V π(s) =PN n=1 qnV π n (s), ∀s ∈ Sof policyπ and the value functionV π I of policyπ in the imaginary MDP. Lemma 3 For all statess and policiesπ, we have¯V π(s) ≥ V π I (s). Lemma 4 For all policiesπ, we have \r\r¯V π − V π I \r\r ≤ γRmaxκ1 (1−γ)2 . Readers are referred to Jin et al. (2022) for detailed proofs and discussions of Lemmas 3 and 4. Briefly,V π I serves as a lower bound of¯V π. Among all policies, the optimal policyπ∗ I in 6Client Selection for Federated Policy Optimization the imaginary MDP is of particular interest since its value functionV π∗ I I = V ∗ I is the largest lower bound of the average value function¯V π. Since there are approximation errors (7) in each client and aggregation error in the central server, we can only obtain an approximation V t of the averaged value function¯V πt with \r\r\rV t − ¯V πt \r\r\r ≤ \r\rV t − ¯V t\r\r + \r\r\r¯V t − ¯V πt \r\r\r ≤ ¯εw + NX n=1 qnδn = ¯δ, where ¯V t(s) = PN n qnV t n(s), ∀s ∈ Sis the average of the local approximation and¯εw = maxt \r\rV t − ¯V t\r\r is the error induced by value function aggregation. Note that we postpone the discussion of the errorεw induced by value function aggregation to the end of this section. Therefore, by the triangle inequality, we have \r\r\rV t − V πt I \r\r\r ≤ γRmaxκ1 (1 − γ)2 + ¯δ = ˙δ. (8) 3.2 Federated Policy Improvement Note that the approximation error in (8) matches that of the policy evaluation of API (4). This motivates us to further obtain an API-style approximation error for the policy improvement phase of FAPI. We consider two variants of FAPI: FAPI with federated policy evaluation (Algorithm 1) and FAPI without federated policy evaluation (Algorithm 2). Algorithm 1 is communication inefficient in practice since it introduces an extra round of communication for policy evaluation (lines 3 - 8 in Algorithm 1). The two algorithms lead to different approximation errors of the policy improvement phase as will be shown by Lemma 5 and Lemma 6, respectively. Algorithm 1FAPI with federated policy evaluation 1: Input: N, T. 2: for t = 0,1,...,Tdo 3: Synchronize the global policyπt to every client. 4: for n = 0,1,...,Ndo 5: Approximate the value functionV πt n with V t n. 6: Upload V t n to the central server. 7: end for 8: The central server aggregatesV t n to obtainV t: wt+1 ← PN n=1 qnwt+1 n . 9: Synchronize the global policyπt and value functionV t to every client. 10: for n = 0,1,...,Ndo 11: Local update of client policy: \r\r\rTπt+1 n n V t − TnV t \r\r\r ≤ ϵn. 12: Upload πt+1 n to the central server. 13: end for 14: The central server aggregatesπt+1 n to obtainπt+1: θt+1 ← PN n=1 qnθt+1 n . 15: end for 7Xie and Song Algorithm 2FAPI without federated policy evaluation 1: Input: N, T. 2: for t = 0,1,...,Tdo 3: Synchronize the global policyπt to every client. 4: for n = 0,1,...,Ndo 5: Approximate the value functionV πt n with V t n. 6: Local update of client policy: \r\r\rTπt+1 n n V t n − TnV t n \r\r\r ≤ ϵn. 7: Upload πt+1 n to the central server. 8: end for 9: The central server aggregatesπt+1 n to obtainπt+1: θt+1 ← PN n=1 qnθt+1 n . 10: end for Lemma 5 With federated policy evaluation, the sequence \u0000 πt\u0001∞ t=0 generated by Algorithm 1 satisfies \r\r\rTπt+1 I V t − TIV t \r\r\r ≤ ¯εθ p |A|Rmax 1 − γ + 2γRmaxκ1 1 − γ + ¯ϵ = ϵ′, where ¯ϵ = PN n=1 qnϵn and ¯εθ = maxt>0,s∈S \r\r\rπt+1(·|s) − PN n=1 qnπt+1 n (·|s) \r\r\r 2 . See Appendix A for the detailed proof. Note that we postpone the discussion of the errorεθ induced by policy aggregation to the end of this section. Lemma 6 Without federated policy evaluation, the sequence \u0000 πt\u0001∞ t=0 generated by Algorithm 2 satisfies \r\r\rTπt+1 I V t − TIV t \r\r\r ≤ ¯εθ p |A|Rmax 1 − γ + 2γ¯εw + 2γ2Rmaxκ2 (1 − γ)2 + γRmaxκ1 1 − γ + 4γ¯δ + ¯ϵ = ˙ϵ. (9) See Appendix B for the detailed proof. As shown by Lemma 5 and Lemma 6, FAPI with federated policy evaluation provides a tighter bound. Intuitively, forcing clients to optimize their local policy from the same starting point (value function¯V πt ) helps them mitigate the negative impact of heterogeneity. While Algorithm 2 is what FRL applications typically employ in practice, it is also more vulnerable to environment heterogeneity as its error bound is inferior to the one of Algorithm 1 roughly by a factor of1 − γ. By far, we have assumed full client participation, i.e., all clients participate in every round of training. However, partial client participation is more favorable in practice. Proposition 7 accounts for this scenario. Proposition 7 Let C denote the set of selected clients andq′ m = qmP m∈C qm . With partial client participation and without federated policy evaluation, the sequence \u0000 πt\u0001∞ t=0 generated by Algorithm 2 satisfies \r\r\rTπt+1 I V t − TIV t \r\r\r ≤ ˜εθ p |A|Rmax 1 − γ + 2γ¯εw + X m∈C NX n=1 q′ mqn \u0000 γ + γ2\u0001 Rmaxκm,n (1 − γ)2 + X m∈C q′ m γRmaxκm,I 1 − γ + 2γ X m∈C q′ mδm + 2γ¯δ + X m∈C q′ mϵm = ˆϵ, (10) 8Client Selection for Federated Policy Optimization where ˜εθ = maxt>0,s∈S \r\rπt(·|s) − P m∈C q′ mπt m(·|s) \r\r 2. See Appendix C for the detailed proof. To better understand how to minimize the right-hand side of (10), one can consider the optimization problem,minx f(x) = 1 N PN n=1 |x − aj|, which corresponds to the caseqn = 1 N , n= 1, ··· , Nand |C| = 1 in (10). It can be easily shown that f(x) is minimal whenx is the median of{a1, ··· , aN }. Remark 8 Proposition 7 reveals a remarkable fact that a proper client selection method can effectively reduce the error bound of the policy improvement phase. In particular, to have the right-hand side of (10) smaller than the right-hand side of (9), the selected clients shall have an averageκm,n that is smaller than 2γ 1+γ κ2. This encourages FAPI to select clients that are closer to the imaginary MDP, which is a reasonable approximation to the median of all transition probabilities. For completeness, we provide the error bound of the policy improvement phase with partial client participation and federated policy evaluation by the following proposition. Proposition 9 LetC denote the set of selected clients andq′ m = qmP m∈C qm . With partial client participation and federated policy evaluation, the sequence \u0000 πt\u0001∞ t=0 generated by Algorithm 1 satisfies \r\r\rTπt+1 I V t − TIV t \r\r\r ≤ ˜εθ p |A|Rmax 1 − γ + X m∈C NX n=1 q′ mqn γRmaxκm,n 1 − γ + X m∈C q′ m γRmaxκm,I 1 − γ + X m∈C q′ mϵm = ´ϵ. (11) See Appendix E for the detailed proof. 3.3 Bounding the distance between value functions The following theorem provides the error bound of FAPI in terms of the distance between value functions. Theorem 10 Let π∗ = arg maxπ η(π). The sequence \u0000 πt\u0001∞ t=0 generated by FAPI satisfies lim sup t→∞ \f\f\f¯V πt (s) − ¯V max s \f\f\f ≤ ˜ϵ + 2γ ˙δ (1 − γ)2 + 2γRmaxκ1 (1 − γ)2 , (12) where ¯V max s = max \b¯V π∗ (s), ¯V π∗ I (s) \t , ∀s ∈ S, and ˜ϵ may be one of ˙ϵ, ϵ′, ˆϵ or ´ϵ. More specifically, the error bound for Algorithm 2 with partial client participation is lim sup t→∞ \f\f\f¯V πt (s) − ¯V max s \f\f\f ≤ C1κ1 + C2 X m∈C q′ mκm,I + C3 X m∈C NX n=1 q′ mqnκm,n + ˜O   ˜εθ p |A|Rmax + ¯εw + ¯δ + X m∈C q′ mδm + X m∈C q′ mϵm ! , (13) where ˜O omits some constants related toγ, C1 = 2γ(γ2−γ+1) (1−γ)4 Rmax, C2 = γ (1−γ)3 Rmax, and C3 = γ+γ2 (1−γ)4 Rmax. 9Xie and Song See Appendix D for the detailed proof. Remark 11 The bound given by Theorem 10 is similar to the one of centralized API as in Proposition 1 and inversely proportional to the heterogeneity levelκ1 and κ2, which explicitly unveils the impact of the data heterogeneity. The second term in (12) stems from the difference between \r\r\r¯V πt − ¯V π∗ \r\r\r and \r\r\rV πt I − V ∗ I \r\r\r. Although the imaginary MDP enables us to analyze the theoretical properties of FAPI and shows the error bound in terms ofV ∗ I , ¯V π∗ is the actual target (refer to (3)) that FAPI wants to achieve. Fortunately, this bound is still useful, since (12) is dominated by its first term. To this end, the optimal policy in the imaginary MDP is a good estimation of the optimal policy for (3) unless there is a bound sharper than ˙δ. Again, the client selection scheme is the key to reducing error. 3.4 Impact of Aggregation Error We have defined three terms to quantify the impact of policy aggregation and value function aggregation, i.e.,˜εθ, ¯εθ and ¯εw. To further investigate how these errors affect the convergence of FAPI, there are two possible approaches: 1) Performing a general analysis with a few standard assumptions in the convex optimization literature; and 2) Carrying out the analysis with a specific neural network parameterization to gain insight into network configuration that can affect the approximation error. In light of the recent breakthrough in overparameterized neural networks (Arora et al., 2019; Cai et al., 2019; Wang et al., 2019; Liu et al., 2019), we employ the second strategy. To that end, we analyze the impact of aggregation error with the following two-layer ReLU-activated neural networks to parameterize the state value function and policy, respectively, as uw(s) = 1√m mX i bi · σ(wT i (s)), (14) fθ(s, a) = 1√m mX i bi · σ(θT i (s, a)), (15) where m is the width of the network,θ = (θi, . . . θm)T ∈ Rm(ds+da), w = (wi, . . . wm)T ∈ Rmds denotes the input weights, andbi ∈ {−1, 1}, ∀i ∈ [m] is the output weight. Without loss of generality, we assume thata ∈ Rda, s∈ Rds, ∥(s, a)∥2 ≤ 1, and denoted = ds + da. Given arbitrary ˆRϑ < ∞, we consider the following parameter initialization, whereϑ is a placeholder forθ and w: Einit \u0002 ϑ0 i,j \u0003 = 0, Einit h\u0000 ϑ0 i,j \u00012i = 1 d · m, ∀i ∈ [m], j∈ [d], bi ∼ Unif({−1, 1}), Einit h\r\rϑ0 i \r\r−2 2 i < ∞, \r\rϑ0 i \r\r 2 ≤ ˆRϑ, ∀i ∈ [m]. (16) In other words,0 < \r\rϑ0 i \r\r 2 ≤ ˆRϑ, ∀i ∈ [m]. The output weightsbi, ∀i ∈ [m] are fixed. The input weightsϑi, ∀i ∈ [m] are projected into a ball centered at the initial parameter, i.e., θ ∈ B0 Rθ = \b θ : \r\rθ − θ0\r\r 2 ≤ Rθ \t , w∈ B0 Rw = \b w : \r\rw − w0\r\r 2 ≤ Rw \t . Then, the state value function and Softmax policy are parameterized by V t(s) = uwt(s), πt(a|s) = exp(fθt(s, a))P a′∈A exp(fθt(s, a′)), ∀a ∈ A, s∈ S. (17) 10Client Selection for Federated Policy Optimization The following lemma quantifies the aggregation error under the above setting. Lemma 12 By utilizing the initialization scheme in (16), the policy parameterization in (17), and the neural network parameterization in (14) and (15), we have Einit [¯εw] = O \u0010 R6/5 w m−1/10 ˆR2/5 w \u0011 , (18) Einit [¯εθ] = O \u0010 R1/2 θ \u0011 , (19) Einit [˜εθ] = O \u0010 R1/2 θ \u0011 . (20) See Appendix G for the detailed proof. Remark 13 Lemma 12 indicates that the aggregation error is determined by the neural network parameterization (m) and optimization method (Rθ and Rw). Moreover, it can be observed from the proof that the error of value function aggregation is minimized when using linear function approximation, e.g., linear regression and tabular implementation, and the error of policy aggregation is minimized when using log-linear policy parameterization, e.g., Softmax policy with linear regression. In other words, there is a trade-off between training (non-linearity) and aggregation error (linearity). A non-linear function approximation can fit the training data well, but suffers from high aggregation error. In contrast, the linear function approximation may not be able to solve complex problems, but it does not introduce aggregation error. 3.5 Connection to Centralized Learning When the environments are homogeneous, the policy is log-linear w.r.t. parameters, and the value function is linear w.r.t. parameters, the learning process of FAPI will be equivalent to learning from N copies of the same environment (that is identical to the imaginary MDP) with federated policy evaluation. Under such circumstances, Theorem 10 degenerates to that for centralized learning (6) (Bertsekas, 2022) with the same error bound, i.e.,¯ϵ−2γ¯δ (1−γ)2 , as shown in the following proposition. Proposition 14 With homogeneous environments and federated policy evaluation, the error bound for partial/full client participation is lim sup t→∞ \r\r\r¯V πt − ¯V π∗ \r\r\r ≤ ˆεθ p |A|Rmax (1 − γ)3 + 2γ¯εw (1 − γ)2 + ¯ϵ + 2γ¯δ (1 − γ)2 , where ˆεθ is equal to¯εθ and ˜εθ for full participation and partial participation, respectively. See Appendix F for the detailed proof. As discussed in Section 3.4,ˆεθ = 0 when the policy is log-linear w.r.t. parameters, and¯εw = 0 when the value function is linear w.r.t. parameters. 4 Federated Policy Optimization with Heterogeneity-aware Client Selection In this section, we propose a federated policy optimization algorithm based on the discus- sions in Section 3. The pseudocode for the proposed Federated Policy Optimization with Heterogeneity-aware Client Selection (FedPOHCS) is illustrated in Algorithm 3. 11Xie and Song 4.1 Client Selection Metric As characterized by Remark 8, a proper client selection method should be able to capture the heterogeneity levelκ1/κ2 of the selected clients. In general, the smaller the difference between the transition probability of selected clients and that of the imaginary MDP, the better bound we may obtain. As there are many methods to approximate and represent the transition probability, we consider it as an implementation consideration and leave it to the applications. The use of transition probability makes the proposed client selection scheme a model-based framework (Moerland et al., 2020). Next, we make several approximations to the theoretically justified client selection metric, i.e., the level of heterogeneity of then-th clientκn,I defined in Definition 2. It is hard to compute κn,I by finding the maximum value over all states since the number of samples is finite in practice and this metric may suffer from high variance. Therefore, we use the current global policy to compute the metric and weight each transition with the stationary (or steady-state) distributiondπ,Pn(s) of the entry states (Bojun, 2020). Moreover, in the proofs of the lemmas and propositions, we relax the inequalities by replacing all value functions with their upper boundRmax 1−γ . To further improve the approximation, we scale each transition with the value (advantage or Q-value) of each state-action pair(s′, a). Then, for each tuple of (s, s′, a) in the n-th client, we have ˆκn,I(s, s′, a) = \f\f\f\f\f\f dπ,Pn(s)Pπ n (s′|s)Aπ,Pn(s′, a) − NX j=1 qjdπ,Pj (s)Pπ j (s′|s)Aπ,Pj (s′, a) \f\f\f\f\f\f . However, it is too expensive to compute all|S| × |S| × |A|elements for each client. Since the stationary distribution is a fixed-point distribution, i.e.,P s dπ,Pn(s)Pπ n (s′|s) = dπ,Pn(s′), we can sum over the first dimension to simplify the metric and reduce the amount of computation. In other words, we want to compute ˆκn,I(s′, a) = \f\f\f\f\f\f X s dπ,Pn(s)Pπ n (s′|s)Aπ,Pn(s′, a) − NX j=1 qj X s dπ,Pj (s)Pπ j (s′|s)Aπ,Pj (s′, a) \f\f\f\f\f\f = \f\f\f\f\f\f dπ,Pn(s′)Aπ,Pn(s′, a) − NX j=1 qjdπ,Pj (s′)Aπ,Pj (s′, a) \f\f\f\f\f\f . For compact notation, we defineDπ,Pn and Dπ,µ,Pn,γ as two|S| × |S|diagonal matrices whosei-thdiagonalentriesarethestationarydistributionanddiscountedvisitationfrequencies of statesi, respectively. We denoteΠπ as a|S| × |A|matrix whose(i, j)th entry isπ(aj|si), and Aπ,Pn as a |S| × |A|matrix whose (i, j)th entry is the advantage of actionaj on state si, i.e., a matrix representation of the advantage function (Kakade and Langford, 2002). Then, we can rewrite the approximated level of heterogeneityP s′,a ˆκn,I(s′, a)2 as ˆκn,I = \r\r\rPN k=1 qkDπ,Pk Aπ,Pk − Dπ,PnAπ,Pn \r\r\r F , where we use the Frobenius norm to make the metric more sensitive to entries with large difference. However, the metricˆκn,I has an obvious drawback, i.e., the algorithm will keep selecting clients that are closer to the imaginary MDP even if those clients are sufficiently trained. 12Client Selection for Federated Policy Optimization As a solution, we propose to consider the learning potential of local policies. In fact, the learning step size of many policy optimization algorithms depends on the magnitude of value functions. For example, the advantage function (or Q-value) can affect the magnitude of policy gradient, and the Q-value can affect the updates in Temporal Difference (TD) methods, including Q-learning and SARSA. This observation motivates us to use the magnitude of the advantage function, i.e.,∥Dπ,PnAπ,Pn∥F , to measure how much then-th client can learn starting from the current global policy. Finally, we define the selection metric of FedPOHCS as ∆n = ∥Dπ,PnAπ,Pn∥F − \r\r\r\r\r NX k=1 qkDπ,Pk Aπ,Pk − Dπ,PnAπ,Pn \r\r\r\r\r F , ∀n = 1, ··· , N. (21) As shown in the first phase (lines 3 - 9) of Algorithm 3, the server samples a candidate set with d clients and computes∆n for all clients in this set. Then, the server selectsK clients with the largest∆n from the candidate set and starts the second phase (lines 10 - 15) of Algorithm 3. As a client selection metric,∆n is better than the original heterogeneity measurementκn,I, because it helps skip clients that can not sufficiently contribute to the global objective (3). This helps allocate resources to clients whose information has not been fully learned instead of investing all resources into those who are just closer to the imaginary MDP throughout the learning process. Similar measurements of heterogeneity and learning potential can be found in FedKL (Xie and Song, 2023), where states are weighted by the discounted visitation frequency instead of the stationary distribution. 4.2 Implementation To compute∆n, we adopt the tabular maximum likelihood model (Moerland et al., 2020; Strehl et al., 2009; Ornik and Topcu, 2021; Strehl and Littman, 2008) in the implementation of FedPOHCS. Given a set of trajectories, letCn(s, a) denote the number of times actiona was taken under states, Cn(s, a, s′) represent the number of times the MDP transited from state s to s′ after taking actiona, andrn denote the corresponding sequence of reward received. Generally speaking, the more trajectories we have, the more accurate the modeling will be. For Mountain Cars and Hoppers, the number of trajectories is 200. For HongKongOSMs, the number of trajectories is 1000. We can estimate the transition probabilityPn of then-th MDP and the reward functionR by ˆPn = Cn(s, a, s′) Cn(s, a) , ˆRn = 1 Cn(s, a) Cn(s,a)X i=1 rn[i]. We estimate the state visitation frequency matrixˆDπ,µ,Pn in place ofDπ,Pn as follows (Ziebart et al., 2008): Dn,s′,0 = µ(s′), D n,s′,t+1 = X s,a Dn,s,tπ(a|s) ˆPn(s′|s, a), D n,s′ = X t Dn,s′,t, where the time horizont is a hyperparameter. Then, we diagonalize the vectorDn,s′ to obtain the state visitation frequency matrixˆDπ,µ,Pn. Last, we estimate the advantage function by Generalized Advantage Estimation (GAE) (Schulman et al., 2016). 13Xie and Song In contrast to client selection schemes that update the selection metrics together with the models at the end of each round, we employ an extra round for metric computation (lines 3 - 9) in Algorithm 3. A similar selection scheme was utilized by a biased client selection method called Power-of-Choice (Jee Cho et al., 2022). The choice of the local learner is optional as our discussion is general and does not rely on any particular implementation of policy evaluation and policy improvement. In particular, we assume all clients adopt the Proximal Policy Optimization (PPO) algorithm proposed by Schulman et al. (2017), which is a PG method motivated by TRPO. Algorithm 3FedPOHCS 1: Input: the initial estimation of the transition probabilitiesˆPk and reward functionsˆRk, d, T, K. 2: for t = 0,1,...,Tdo 3: Sample the candidate client setC of d (K ≤ d ≤ N) clients without replacement. 4: Synchronize the global policyπt to every selected client. 5: for k ∈ Cdo 6: Compute the advantage functionAπ,Pk and the state visitation frequencyˆDπt,µ,Pk . 7: Upload Aπ,Pk , ˆDπt,µ,Pk to the central server. 8: end for 9: Compute ∆k, ∀k = 1, ..., d, Select K clients based on∆k and replace C with these clients. 10: for k ∈ Cdo 11: Local update of client policyπt+1 k , transition probabilityˆPk, and reward function ˆRk. 12: Upload πt+1 k to the central server. 13: end for 14: The central server performs aggregation:πt+1(a|s) ← PK k=1 qkπt+1 k (a|s), ∀s ∈ S, a∈ A. 15: end for 4.3 Limitations of the Proposed Implementation Algorithm 3 utilizes a two-phase communication scheme which is not communication-efficient unless the convergence improvement obtained by client selection suppresses this cost. However, we note that such a communication cost can be removed by using outdated information to compute the selection metrics at the cost of accuracy as in Jee Cho et al. (2022). In particular, we can remove the first phase, and order selected clients to upload their local information (e.g., Aπ,Pn and Dπ,µ,Pn matrices) when uploading their models at the end of each communication round. This communication-efficient variant saves a lot of communication overhead and has been shown to be effective by [1], i.e., with slightly worse performance. The performance of the one-phase scheme will be shown later in the experiment results. Another limitation of Algorithm 3 is that it requires the clients to upload their state visitation frequencies ˆDπ,µ,Pn and advantage functionsAπ,Pn which may contain private in- 14Client Selection for Federated Policy Optimization formation. This problem can be addressed by privacy protection methods, e.g., Homomorphic Encryption (HE) (Jiang et al., 2018). 5 Experiments In this section, we introduce two federated environments for empirical evaluation and evaluate the proposed client selection method from different perspectives. All experimental results are reproducible and can be accessed from: https://github.com/ShiehShieh/FedPOHCS. 5.1 Environments We evaluate the effectiveness of the proposed client selection algorithm on a federated version of the mountain car continuous control problem (Moore, 1990) and the Mujoco Hopper problem (Coulom, 2002). Furthermore, we utilize the Flow simulator Vinitsky et al. (2018) and OpenStreetMap (OSM) (OpenStreetMap contributors, 2017; Vargas-Munoz et al., 2021) dataset to create a series of traffic networks for autonomous vehicle training. We construct the federated environments as follows: Mountain Carsconsists of 60 equally weighted MountainCarContinuous-v0 environ- ments developed by OpenAI Gym (Brockman et al., 2016). In each environment, a car aims to reach a hill. The episode terminates when the car reaches this hill or runs out of time. The environment consists of a 2-dimensional continuous state space and a 1-dimensional continuous action space. To introduce heterogeneity into the system, we assume that the engine of each car is different and then-th car shifts the input action byθn on all states. To introduce a medium-level heterogeneity, the constant shiftθn is uniformly sampled from [−1.5, 1.5] and assigned to each environment at initialization. In fact, to make the experi- ments traceable, we set the constant shift toθn = −1.5 + n 20, n= 1, . . . ,60. The intervals for low-level and high-level heterogeneity are[−1.0, 1.0] and [−2.0, 2.0], respectively. Hoppers consists of 60 equally weighted Hopper-v3 environments developed by OpenAI Gym. The environment has an 11-dimensional continuous state space and a 3-dimensional continuous action space. We introduce the heterogeneity into this system by following Jin et al. (2022), i.e., the leg size is uniformly sampled from[0.01, 0.07], [0.01, 0.10], and [0.01, 0.15] for low-level, medium-level, and high-level heterogeneity, respectively. HongKongOSMs consists of 10 equally weighted traffic networks, each based on one of the OSM datasets as shown in Figure 1. Each traffic network contains one RL-controlled and 10 IDM-controlled (Intelligent Driver Model) vehicles. The 18-dimensional observation includes headway, speed, and positional information of visible neighborhoods of the RL- controlled vehicle, and only observations for vehicles running on the same and adjacent lanes are visible to the RL-controlled vehicle. The 2-dimensional action includes acceleration and lane-changing decisions. We adopted the Eclipse Simulation of Urban MObility (SUMO) simulator to conduct this experiment. 5.2 Experiment Settings We use neural networks to represent policies as in Schulman et al. (2015); Vinitsky et al. (2018). Specifically, we use Multilayer Perceptrons (MLPs) withtanh non-linearity and hidden layers (64, 64). We use the SGD optimizer with a momentum of 0.9 and learning-rate 15Xie and Song (a)  (b)  (c)  (d)  (e) (f)  (g)  (h)  (i)  (j) Figure 1: OSM datasets of ten areas in Hong Kong. (a) Causeway; (b) Central; (c) Chai Wan; (d) Clear Water Bay; (e) Kennedy Town; (f) Kai Tak; (g) North Point; (h) Po Lam; (i) Sham Shui Po; (j) Tseung Kwan O. decay of 0.98, 0.9, and 0.98 per round for Mountain Cars, Hoppers, and HongKongOSMs, respectively. Hyperparameters are carefully tuned so that they are near-optimal for each algorithm. See Appendix H for more experimental details. We compare FedPOHCS with biased and unbiased client selection methods, including FedAvg (random selection), Power-of-Choice, and GradientNorm (Chen et al., 2021; Marnissi et al., 2021; Chen et al., 2022). FedAvg randomly selects K clients. Power-of-Choice utilizes the aforementioned two-phase scheme and selects candidate clients with the highest loss (or lowest advantages/values in case of RL problem). GradientNorm selects candidate clients with the largest gradient norm. Besides the client selection scheme, all algorithms follow the same procedure described at the beginning of Section 2.1. In particular, clients perform local training with the algorithm proposed by Schulman et al. (2017), with adaptive KL penalty term. At the end of every round, the server broadcasts the global policy to all clients, orders them to interact with their MDPs for several episodes (10 for Mountain Cars, 100 for Hoppers, and 20 for HongKongOSMs) and report the mean returns to evaluate the performance. Each experiment is averaged across three independent runs with different random seeds and parameter initializations, both of which are shared among all algorithms for a fair comparison. Confidence intervals are also reported. 16Client Selection for Federated Policy Optimization Figure 2: Comparison of FedAvg, Power-of-Choice, FedPOHCS, and GradientNorm on Mountain Cars with a medium level of heterogeneity. For FedAvg, the learning rate is 0.005 and the KL target is 0.003. For Power-of-Choice, GradientNorm, and FedPOHCS, the learning rate is 1e-3, and the KL target is 0.003. Figure 3: Comparison of FedAvg, Power-of-Choice, FedPOHCS, and GradientNorm on Hoppers with a medium level of heterogeneity. For all algorithms, the learning rate is 0.03, the learning rate decay is 0.9, and the KL target is 0.003. 5.3 Performance and Stability Although the original mountain car problem is simple and most modern RL algorithms can easily obtain a score over 90.0, the federated setting imposes great difficulties in solving it. Figure 2 shows the performance comparison between FedPOHCS and several baselines on Mountain Cars with medium-level heterogeneity. It can be observed that FedPOHCS has a faster convergence speed and a more stable learning process. To compute the selection metric, we discretize the state and action by rounding them off to the nearest 0.1, resulting in about 8000 states and 100 actions that are frequently visited. In each round, the first phase of FedPOHCS takes 1-10 seconds, and the local training takes 20-30 seconds. Although the running time of the first phase may vary depending on the implementation, FedPOHCS 17Xie and Song Figure 4: Comparison of FedAvg, Power-of-Choice, FedPOHCS, and GradientNorm on HongKongOSMs. For all algorithms, the learning rate is 0.0001, the learning rate decay is 0.98, and the KL target is 0.0001. outperforms all baselines in terms of the number of rounds and wall-clock time in our setting. In Figure 2, we have also included the communication-efficient variant, i.e., the one-phase scheme, for comparison purposes. It can be observed that the one-phase scheme may slow down and destabilize learning in the early stage of training, but the final performance is comparable to the two-phase scheme. We can draw a similar conclusion on Hoppers with medium-level heterogeneity. As shown in Figure 3, FedPOHCS can obtain an accumulated reward of 1450 within 20 rounds of training, while it takes about 80 rounds for Power-of-Choice to reach 1450, demonstrating the advantage of the proposed FedPOHCS algorithm in speeding up convergence. HongKongOSMs is much more difficult to solve as shown by the high-variance curves in Figure 4. Since different OSM datasets have different numbers of lanes, target velocity, and maximum acceleration/deceleration, their state spaces and transition probabilities may be highly distinct from each other. Compared with other approaches, FedPOHCS can obtain higher rewards and converge to the highest point. 5.4 Effectiveness of Metrics In Figure 5, we show how different algorithms select clients (the histogram) and the reward obtained by the final policy from each client (the scatter points) on Mountain Cars with medium-level heterogeneity. Note that with the constant shiftθn = −1.5 + n 20, clients with small IDs are very different from those with large IDs. It can be observed that, compared with random selection and Power-of-Choice, FedPOHCS refuses to allocate resources to clients with IDs in[40, 60], while Power-of-Choice spends a significant amount of resources on them. The final policy generated by FedPOHCS performs well on almost all clients and gets fairly high scores on clients with IDs in[1, 10] without hurting other clients. This indicates that clients with IDs in[1, 10] and clients with IDs in[40, 60] are competing with each other. This is because they have constant shiftsθn of different directions. Moreover, policies that perform well on clients with IDs in[1, 10] may get fairly good scores on clients with IDs in 18Client Selection for Federated Policy Optimization Figure 5: The histogram represents the frequency of each client being selected. The scatter points denote the return obtained by the final policy from each client. (top) FedPOHCS, (middle) FedAvg, and (bottom) Powder-of-Choice. [40, 60], while the opposite is not true. We conjecture that the relation between the constant shifts θn and the transition probabilities is not linear and the imaginary MDP is closer to clients with small IDs, and hence learning on clients with IDs in[40, 60] can be harmful to the overall performance. 19Xie and Song (a) (b) (c) Figure 6: Comparison of FedAvg, Power-of-Choice, FedPOHCS and GradientNorm on Moun- tain Cars with low/medium/high level of heterogeneity. 20Client Selection for Federated Policy Optimization (a) (b) (c) Figure 7: Comparison of FedAvg, Power-of-Choice, FedPOHCS and GradientNorm on Hop- pers with low/medium/high level of heterogeneity. 5.5 Different levels of heterogeneity In Figure 6, we show the performance comparison with different levels of heterogeneity, i.e., low (a), medium (b), and high (c), on Mountain Cars. It can be observed that, as the 21Xie and Song level of heterogeneity increases, the performance of all algorithms decreases, but the gap between FedPOHCS and the others increases. In other words, FedPOHCS demonstrates a bigger advantage when the level of heterogeneity is high, though its performance is also affected by severe heterogeneity. We have also conducted similar experiments on Hoppers as shown in Figure 7. While we can draw the same conclusion on FedAvg, i.e., its performance decreases as the level of heterogeneity increases, all three biased client selection methods are less affected by the level of heterogeneity. 6 Conclusion In this work, we derived an error bound for federated policy optimization that explicitly unveils the impact of environment heterogeneity. The associated analysis covered various scenarios in FRL and offered insights into the effects of different federated settings. In particular, it was shown that clients whose environment dynamics are close to the population distribution are preferable for training. Based on these results, a client selection algorithm was proposed for FRL with heterogeneous clients. Experiment results demonstrated that the proposed client selection scheme outperforms other baselines on two federated RL problems. The results of this work represent a small step in understanding FRL and may motivate further research efforts in client selection for FRL. Acknowledgments and Disclosure of Funding This work was fully supported by a grant from the NSFC/RGC Joint Research Scheme sponsored by the Research Grants Council of the Hong Kong Special Administrative Region, China and National Natural Science Foundation of China (Project No. N_HKUST656/22). 22Client Selection for Federated Policy Optimization APPENDICES This appendix contains the proof of Lemma 5 in Appendix A, the proof of Lemma 6 in Appendix B, the proof of Proposition 7 in Appendix C, the proof of Theorem 10 in Appendix D, the proof of Proposition 9 in Appendix E, the proof of Proposition 14 in Appendix F, and additional experiment setting in Appendix H. Appendix A. Proof of Lemma 5 The following Lemmas will be frequently used throughout the appendix. Lemma 15 For any value functionV ∈ R|S|, policyπ and clientn, m, we have ∥TmV − TnV ∥ ≤γRmaxκm,n 1 − γ , (22) ∥TIV − TnV ∥ ≤γRmaxκn,I 1 − γ , (23) ∥Tπ mV − Tπ n V ∥ ≤γRmaxκm,n 1 − γ , (24) ∥Tπ I V − Tπ n V ∥ ≤γRmaxκn,I 1 − γ . (25) Proof For anys ∈ Sand m, n= 1, . . . , N, leta_ = arg maxa R(s, a)+ γ P s′ ¯P(s′|s, a)V (s′), an = arg maxa R(s, a)+γ P s′ Pn(s′|s, a)V (s′), andam = arg maxa R(s, a)+γ P s′ Pm(s′|s, a)V (s′) denote the greedy actions taken byTI, Tn and Tm on V (s), respectively. Then, for any state s, we have |TmV (s) − TnV (s)| = \f\f\f\f\fR(s, am) − R(s, an) + γ X s′ \u0000 Pm(s′|s, am) − Pn(s′|s, an) \u0001 V (s′) \f\f\f\f\f. Without loss of generality, we assume thatTmV (s) ≥ TnV (s). As a result, we can obtain the following inequality by replacingan with am |TmV (s) − TnV (s)| ≤ \f\f\f\f\fγ X s′ \u0000 Pm(s′|s, am) − Pn(s′|s, am) \u0001 V (s′) \f\f\f\f\f ≤ γ X s′ \f\f\u0000 Pm(s′|s, am) − Pn(s′|s, am) \u0001\f\f\f\fV (s′) \f\f ≤ γRmaxκm,n 1 − γ , where the last inequality follows from the fact that all value functions are bounded byRmax 1−γ . This completes the proof of (22) and the proof of (23) is similar. Next, we prove (24). By 23Xie and Song following a similar procedure, we can obtain |Tπ mV (s) − Tπ n V (s)| = \f\f\f\f\f\f γ X a,s′ π(a|s) \u0000 Pm(s′|s, a) − Pn(s′|s, a) \u0001 V (s′) \f\f\f\f\f\f ≤ γ X s′ \f\f\u0000 Pπ m(s′|s) − Pπ n (s′|s) \u0001 V (s′) \f\f ≤ γ X s′ \f\fPπ m(s′|s) − Pπ n (s′|s) \f\f\f\fV (s′) \f\f ≤ γRmaxκm,n 1 − γ . This completes the proof of (24), and the proof of (25) is similar. Lemma 16 Let ¯πt(a|s) = PN n=1 qnπt n(a|s) denote the expected output of all local policies and ˜πt(a|s) = P m∈C q′ mπt m(a|s), ∀s ∈ S, a∈ Arepresent the expected output of a setC of local policies. Define¯εθ = maxt \r\rπt(·|s) − ¯πt n(·|s) \r\r 2 and ˜εθ = maxt \r\rπt(·|s) − ˜πt m(·|s) \r\r 2 for full participation and partial participation, respectively. For any value functionV ∈ R|S| and policy πt at roundt, we have \r\r\rTπt I V − T ¯πt I V \r\r\r ≤ ¯εθ p |A|Rmax 1 − γ , (26) \r\r\rTπt n V − T ¯πt n V \r\r\r ≤ ¯εθ p |A|Rmax 1 − γ , (27) \r\r\rTπt I V − T ˜πt I V \r\r\r ≤ ˜εθ p |A|Rmax 1 − γ , (28) \r\r\rTπt n V − T ˜πt n V \r\r\r ≤ ˜εθ p |A|Rmax 1 − γ . (29) Proof For any states, we have \f\f\fTπt I V t(s) − T ¯πt I V t(s) \f\f\f = \f\f\f\f\f X a NX n=1 qn \u0000 πt(a|s) − πt n(a|s) \u0001   R(s, a) + γ X s′ ¯P(s′|s, a)V t(s′) !\f\f\f\f\f ≤ \r\r\r\r\rπt(·|s) − NX n=1 qnπt n(·|s) \r\r\r\r\r 2 \r\r\r\r\rR(s, ·) + γ X s′ ¯P(s′|s, ·)V t(s′) \r\r\r\r\r 2 ≤ p |A|Rmax 1 − γ \r\r\r\r\rπt(·|s) − NX n=1 qnπt n(·|s) \r\r\r\r\r 2 ≤ ¯εθ p |A|Rmax 1 − γ , which completes the proof of (26) and the proofs of (27), (28), and (29) are similar. 24Client Selection for Federated Policy Optimization Next, we prove Lemma 5. Proof Let ¯πt(a|s) = PN n=1 qnπt n(a|s) denote the expected output of all local policies. For any states, we have \f\f\fTπt+1 I V t(s) − TIV t(s) \f\f\f ≤ \f\f\fTπt+1 I V t(s) − T ¯πt+1 I V t(s) \f\f\f + \f\f\fT ¯πt+1 I V t(s) − TIV t(s) \f\f\f. (30) By Lemma 16, the first term on the right-hand side (RHS) of (30) is upper bounded by \f\f\fTπt+1 I V t(s) − T ¯πt+1 I V t(s) \f\f\f ≤ ¯εθ p |A|Rmax 1 − γ . (31) Now we bound the second term on the RHS of (30). For any states, we have \f\f\fT ¯πt+1 I V t(s) − TIV t(s) \f\f\f = \f\f\f\f\f X a ¯πt+1(a|s)   R(s, a) + γ X s′ ¯P(s′|s, a)V t(s′) ! − TIV t \f\f\f\f\f (a) = \f\f\f\f\f NX n=1 qn \"X a πt+1 n (a|s)   R(s, a) + γ X s′ ¯P(s′|s, a)V t(s′) ! − TIV t #\f\f\f\f\f (b) = \f\f\f\f\f\f NX n=1 qn  γ X a,s′ πt+1 n (a|s) \u0000 ¯P(s′|s, a) − Pn(s′|s, a) \u0001 V t(s′) + Tπt+1 n n V t(s) − TIV t   \f\f\f\f\f\f (c) ≤ \f\f\f\f\f\f NX n=1 qnγ X a,s′ πt+1 n (a|s) \u0000 ¯P(s′|s, a) − Pn(s′|s, a) \u0001 V t(s′) \f\f\f\f\f\f + \f\f\f\f\f NX n=1 qn \u0010 Tπt+1 n n V t(s) − TnV t(s) \u0011\f\f\f\f\f + \f\f\f\f\f NX n=1 qn \u0000 TnV t(s) − TIV t(s) \u0001 \f\f\f\f\f. (32) Step (a) follows from¯πt+1(a|s) = PN n=1 qnπt+1 n (a|s), ∀s ∈ S, a∈ A. In step (b), we added and then subtracted the termP a,s′ πt+1 n (a|s)Pn(s′|s, a)V t(s′). The added term is combined with P a,s′ πt+1 n (a|s)R(s, a) to formTπt+1 n n V t(s). Step (c) follows from the triangle inequality. By Lemma 15, the first term on the RHS of (32) is upper bounded by \f\f\f\f\f\f NX n=1 qnγ X a,s′ πt+1 n (a|s) \u0000 ¯P(s′|s, a) − Pn(s′|s, a) \u0001 V t(s′) \f\f\f\f\f\f = \f\f\f\f\f NX n=1 qn \u0010 Tπt+1 n I V t(s) − Tπt+1 n n V t(s) \u0011\f\f\f\f\f ≤ γRmaxκ1 1 − γ . (33) 25Xie and Song By (7), the second term on the RHS of (32) is upper bounded by¯ϵ. By Lemma 15, the third term on the RHS of (32) is upper bounded by \f\f\f\f\f NX n=1 qn \u0000 TnV t(s) − TIV t(s) \u0001 \f\f\f\f\f ≤ γRmaxκ1 1 − γ . (34) By substituting the above-mentioned three upper bounds into (32), we can obtain \f\f\fTπt+1 I V t(s) − TIV t(s) \f\f\f ≤ 2γRmaxκ1 1 − γ + ¯ϵ. (35) By substituting (35) and (31) into (30), we can obtain \f\f\fTπt+1 I V t(s) − TIV t(s) \f\f\f ≤ ¯εθ p |A|Rmax 1 − γ + 2γRmaxκ1 1 − γ + ¯ϵ. (36) Appendix B. Proof of Lemma 6 To prove Lemma 6, we first introduce Lemma 17. Lemma 17 For any states, policyπ and clientsm, n, we have |V π m(s) − V π n (s)| ≤γRmaxκm,n (1 − γ)2 . Proof For any statess, the distance betweenV π m(s) and V π n (s) can be bounded as |V π m(s) − V π n (s)| (a) = \f\f\f\f\f X a π(a|s)   R(s, a) + γ X s′∈S Pm(s′|s, a)V π m(s′) − R(s, a) − γ X s′∈S Pn(s′|s, a)V π n (s′) !\f\f\f\f\f (b) = \f\f\f\f\fγ X s′∈S \u0000 Pπ m(s′|s)V π m(s′) − Pπ m(s′|s)V π n (s′) + Pπ m(s′|s)V π n (s′) − Pπ n (s′|s)V π n (s′) \u0001 \f\f\f\f\f (c) ≤ γ X s′∈S Pπ m(s′|s) \f\fV π m(s′) − V π n (s′) \f\f + γ X s′∈S \f\fPπ m(s′|s) − Pπ n (s′|s) \f\f\f\fV π n (s′) \f\f (d) ≤ γ max s′ \f\fV π m(s′) − V π n (s′) \f\f + γRmax P s′∈S |Pπ m(s′|s) − Pπ n (s′|s)| 1 − γ . (37) Step (a) follows from Bellman’s equation. In step (b), we added and then subtracted the term Pπ m(s′|s)V π n (s′). Step (c) follows from the triangle inequality. Step (d) follows from the fact that all value functions are bounded byRmax 1−γ . By taking the maximum of both sides of (37) over states and after some mathematical manipulations, we can finally obtain |V π m(s) − V π n (s)| ≤γRmaxκm,n (1 − γ)2 . 26Client Selection for Federated Policy Optimization Note that we proved Lemma 17 for the state-value function, and a similar result for the action-value function was given by Lemma 1 in (Strehl and Littman, 2008). Next, we prove Lemma 6. Proof By the triangle inequality, we have \r\r\rTπt+1 I V t − TIV t \r\r\r ≤ \r\r\rTπt+1 I V t − T ¯πt+1 I V t \r\r\r + \r\r\rT ¯πt+1 I V t − T ¯πt+1 I ¯V t \r\r\r + \r\r\rT ¯πt+1 I ¯V t − TI ¯V t \r\r\r + \r\rTI ¯V t − TIV t\r\r, from which we can obtain \r\r\rTπt+1 I V t − TIV t \r\r\r ≤ ¯εθ p |A|Rmax 1 − γ + 2γ¯εw + \r\r\rT ¯πt+1 I ¯V t − TI ¯V t \r\r\r, (38) by Lemma 16, the contraction property of the Bellman operators, and the definition of¯εw. To finish the proof, it suffices to bound the third term on the RHS of (38). By the definition of the Bellman operators, we have \r\r\rT ¯πt+1 I ¯V t − TI ¯V t \r\r\r = \r\r\r\r\r NX n=1 qn \u0010 Tπt+1 n I ¯V t − Tn ¯V t \u0011\r\r\r\r\r ≤ NX n=1 qn \r\r\rTπt+1 n I ¯V t − Tn ¯V t \r\r\r. (39) Next, we further bound the RHS of (39). By the triangle inequality, we have \r\r\rTπt+1 n I ¯V t − Tn ¯V t \r\r\r ≤ \f\f\fTπt+1 n I ¯V t − Tπt+1 n I V t n \r\r\r + \r\r\rTπt+1 n I V t n − TnV t n \r\r\r + \r\rTnV t n − Tn ¯V t\r\r, from which we can obtain \r\r\rTπt+1 n I ¯V t − Tn ¯V t \r\r\r ≤ 2γ \r\r¯V t − V t n \r\r + \r\r\rTπt+1 n I V t n − TnV t n \r\r\r (40) by the contraction property of the Bellman operators. Next, we bound the first term on the RHS of (40). For any states, we have \f\f¯V t(s) − V t n(s) \f\f = \f\f\f\f\f\f NX j=1 qj \u0000 V t n(s) − V t j (s) \u0001 \f\f\f\f\f\f ≤ NX j=1 qj \f\f\fV πt n (s) − V πt j (s) \f\f\f + \f\f\fV πt n (s) − V t n(s) \f\f\f + NX j=1 qj \f\f\fV πt j (s) − V t j (s) \f\f\f 27Xie and Song due to the triangle inequality. By Lemma 17 and (7), we can further obtain \f\f¯V t(s) − V t n(s) \f\f ≤ NX j=1 qj γRmaxκn,j (1 − γ)2 + ¯δ + δn. Thus, we have the following bound \r\r¯V t − V t n \r\r ≤ NX j=1 qj γRmaxκn,j (1 − γ)2 + ¯δ + δn. (41) Now we bound the second term on the RHS of (40). For anys ∈ S, we have \f\f\fTπt+1 n I V t n(s) − TnV t n(s) \f\f\f ≤ \f\f\fTπt+1 n I V t n(s) − Tπt+1 n n V t n(s) \f\f\f + \f\f\fTπt+1 n n V t n(s) − TnV t n(s) \f\f\f ≤ γRmaxκn,I 1 − γ + ϵn, where the last inequality follows from Lemma 15 and (7). Thus, we can obtain \r\r\rTπt+1 n I V t n − TnV t n \r\r\r ≤ γRmaxκn,I 1 − γ + ϵn. (42) By substituting (41) and (42) into (40), and then substituting (40) into (39), we have \r\r\rTπt+1 I ¯V t − TI ¯V t \r\r\r ≤ 2γ2Rmaxκ2 (1 − γ)2 + γRmaxκ1 1 − γ + 4γ¯δ + ¯ϵ. (43) By substituting (43) into (38), we can obtain \r\r\rTπt+1 I V t − TIV t \r\r\r ≤ ¯εθ p |A|Rmax 1 − γ + 2γ¯εw + 2γ2Rmaxκ2 (1 − γ)2 + γRmaxκ1 1 − γ + 4γ¯δ + ¯ϵ. Appendix C. Proof of Proposition 7 Proof Let ˜πt+1(a|s) = P m∈C q′ mπt+1 m (a|s), ∀s ∈ S, a∈ Adenote the expected output of a set C of local policies. By the triangle inequality, we have \r\r\rTπt+1 I V t − TIV t \r\r\r ≤ \r\r\rTπt+1 I V t − T ¯πt+1 I V t \r\r\r + \r\r\rT ˜πt+1 I V t − T ˜πt+1 I ¯V t \r\r\r + \r\r\rT ˜πt+1 I ¯V t − TI ¯V t \r\r\r + \r\rTI ¯V t − TIV t\r\r, from which we can obtain \r\r\rTπt+1 I V t − TIV t \r\r\r ≤ ˜εθ p |A|Rmax 1 − γ + 2γ¯εw + \r\r\rT ˜πt+1 I ¯V t − TI ¯V t \r\r\r, (44) 28Client Selection for Federated Policy Optimization by Lemma 16, the contraction property of the Bellman operators, and the definition of¯εw. To finish the proof, it suffices to bound the third term on the RHS of (44). By the definition of the Bellman operators, we have \r\r\rT ˜πt+1 I ¯V t − TI ¯V t \r\r\r = \r\r\r\r\r X m∈C q′ m NX n=1 qn \u0010 Tπt+1 m I ¯V t − Tn ¯V t \u0011\r\r\r\r\r ≤ X m∈C q′ m NX n=1 qn \r\r\rTπt+1 m I ¯V t − Tn ¯V t \r\r\r. (45) The RHS of (45) can be further bounded by \r\r\rTπt+1 m I ¯V t − Tn ¯V t \r\r\r ≤ \r\r\rTπt+1 m I ¯V t − Tπt+1 m I V t m \r\r\r + \r\r\rTπt+1 m I V t m − TmV t m \r\r\r + \r\rTmV t m − TnV t m \r\r + \r\rTnV t m − Tn ¯V t\r\r (a) ≤ 2γ \u0010\r\r\r¯V t − V πt m \r\r\r + \r\r\rV πt m − V t m \r\r\r \u0011 + \r\r\rTπt+1 m I V t m − Tπt+1 m m V t m \r\r\r + \r\r\rTπt+1 m m V t m − TmV t m \r\r\r + \r\rTmV t m − TnV t m \r\r (b) ≤ NX j=1 qj 2γ2Rmaxκm,j (1 − γ)2 + 2γ¯δ + 2γδm + ϵm + γRmaxκm,n 1 − γ + γRmaxκm,I 1 − γ , (46) where step (a) follows from the contraction property of the Bellman operators and the triangle inequality. Step (b) follows from Lemma 15 and (7). Thus, by substituting (46) into the RHS of (45), we can obtain \r\r\rTπt+1 I ¯V πt − TI ¯V πt \r\r\r ≤ X m∈C NX n=1 q′ mqn \r\r\rTπt+1 m I ¯V πt − Tn ¯V πt \r\r\r ≤ X m∈C NX n=1 q′ mqn \u0000 γ + γ2\u0001 Rmaxκm,n (1 − γ)2 + X m∈C q′ m γRmaxκm,I 1 − γ + 2γ X m∈C q′ mδm + 2γ¯δ + X m∈C q′ mϵm. (47) By substituting (47) into (44), we can conclude \r\r\rTπt+1 I V t − TIV t \r\r\r ≤ ˜εθ p |A|Rmax 1 − γ + 2γ¯εw + X m∈C NX n=1 q′ mqn \u0000 γ + γ2\u0001 Rmaxκm,n (1 − γ)2 + X m∈C q′ m γRmaxκm,I 1 − γ + 2γ X m∈C q′ mδm + 2γ¯δ + X m∈C q′ mϵm. 29Xie and Song Appendix D. Proof of Theorem 10 Proof By the triangle inequality, for any states, we have \f\f\f¯V πt (s) − ¯V π∗ (s) \f\f\f ≤ \f\f\f¯V πt (s) − V πt I (s) \f\f\f + \f\f\fV πt I (s) − V π∗ I I (s) \f\f\f + \f\f\fV π∗ I I (s) − ¯V π∗ (s) \f\f\f, (48) where the first and second terms on the RHS of (48) can be upper bounded by Lemma 4 and Proposition 1, respectively. To bound the third term on the RHS of (48), we notice that, for certain states, the distance between the value functionV π∗ I I = V ∗ I for the optimal policyπ∗ I in the imaginary MDP MI and the average value function¯V π∗ of the optimal policyπ∗ for (3) is upper bounded. Specifically, for states such that ¯V π∗ (s) ≥ ¯V π∗ I (s), we have \f\f\fV π∗ I I (s) − ¯V π∗ (s) \f\f\f ≤ \f\f\fV π∗ I (s) − ¯V π∗ (s) \f\f\f ≤ γRmaxκ1 (1 − γ)2 . (49) By substituting (49), Lemma 4 and Proposition 1 into (48), for a given state s with ¯V π∗ (s) ≥ ¯V π∗ I (s), we have lim sup t→∞ \f\f\f¯V πt (s) − ¯V π∗ (s) \f\f\f ≤ γRmaxκ1 (1 − γ)2 + ˜ϵ + 2γ ˙δ (1 − γ)2 + γRmaxκ1 (1 − γ)2 , (50) where ˜ϵ may be one of˙ϵ (Lemma 6),ϵ′ (Lemma 5),ˆϵ (Proposition 7) or´ϵ (Proposition 9). When the aforementioned condition does not hold, we can alternatively bound the distance between ¯V πt (s) and ¯V π∗ I (s) as \f\f\f¯V πt (s) − ¯V π∗ I (s) \f\f\f ≤ \f\f\f¯V πt (s) − V πt I (s) \f\f\f + \f\f\fV πt I (s) − V π∗ I I (s) \f\f\f + \f\f\fV π∗ I I (s) − ¯V π∗ I (s) \f\f\f, (51) and actuallyπ∗ I performs better on these states. By substituting Lemma 4 and Proposition 1 into (51), for a given states with ¯V π∗ (s) ≤ ¯V π∗ I (s), we have lim sup t→∞ \f\f\f¯V πt (s) − ¯V π∗ I (s) \f\f\f ≤ γRmaxκ1 (1 − γ)2 + ˜ϵ + 2γ ˙δ (1 − γ)2 + γRmaxκ1 (1 − γ)2 . (52) Let ¯V max s = max \b¯V π∗ (s), ¯V π∗ I (s) \t , ∀s ∈ S. We can then combine (50) and (52) into lim sup t→∞ \f\f\f¯V πt (s) − ¯V max s \f\f\f ≤ ˜ϵ + 2γ ˙δ (1 − γ)2 + 2γRmaxκ1 (1 − γ)2 , 30Client Selection for Federated Policy Optimization which proves (12) in Theorem 10. The error bound for Algorithm 2 with partial client participation can be obtained by replacing˜ϵ with ˆϵ as lim sup t→∞ \f\f\f¯V πt (s) − ¯V max s \f\f\f ≤ ˆϵ + 2γ ˙δ (1 − γ)2 + 2γRmaxκ1 (1 − γ)2 = 2γ(γ2 − γ + 1) (1 − γ)4 Rmaxκ1 + γ (1 − γ)3 Rmax X m∈C q′ mκm,I + γ + γ2 (1 − γ)4 Rmax X m∈C NX n=1 q′ mqnκm,n + ˜εθ p |A|Rmax (1 − γ)3 + 2γ¯εw (1 − γ)2 + ˜O   ¯δ + X m∈C q′ mδm + X m∈C q′ mϵm ! , where ˜O omits some constants related toγ. This proves (13) in Theorem 10. D.1 Interpretation of the Error Bound It is difficult to analyze FAPI because we can not directly apply the results of API to FAPI. Fortunately, the use of the imaginary MDPMI aligns FAPI with API and enables the application of Proposition 1 in Theorem 10. However, the imaginary MDPMI also brings two problems: (1) Whileπ∗ is the optimal policy for the objective function of FRL (3), Proposition 1 can only show how far the generated policyπt is from the optimal policyπ∗ I in the imaginary MDP (refer to Remark 11); and (2) The performance (V π I ) of any policyπ in the imaginary MDPMI does not reflect its performance (¯V π) on FRL (3). These problems make the proof intractable as we have to bound the distance between¯V π∗ and V π∗ I I , which is not always feasible. As shown in (49), their distance on a given states is bounded only when¯V π∗ (s) ≥ ¯V π∗ I (s). The difficulty of bounding their distance on all states stems from the fact that the optimal policyπ∗ for (3) does not necessarily outperform other policies on every state (i.e., it may not be uniformly the best). For the states whereπ∗ I outperforms π∗, we can alternatively bound the error with respect to ¯V π∗ I . Although ¯V π∗ I is not directly related to the objective function of FRL (3), it can be regarded as an approximation to¯V π∗ , especially when the level of heterogeneity is low where ¯V π∗ I is close to¯V π∗ . As a result, the error bound in (12) is a combination of two bounds with respect to¯V π∗ I and ¯V π∗ , respectively. An illustrative example is given in Figure 8. 31Xie and Song Figure 8: The area in grey indicates the error bound in (12). The maximum of the two curves on each state is highlighted in red, and the bound is drawn with respect to this highlighted curve. Appendix E. Proof of Proposition 9 Proof Let ˜πt+1(a|s) = P m∈C q′ mπt+1 m (a|s), ∀s ∈ S, a∈ Adenote the expected output of a set C of local policies. For any states, we have \f\f\fTπt+1 I V t(s) − TIV t(s) \f\f\f ≤ \f\f\fTπt+1 I V t(s) − T ¯πt+1 I V t(s) \f\f\f + \f\f\fT ¯πt+1 I V t(s) − TIV t(s) \f\f\f. (53) By Lemma 16, the first term on the right-hand side (RHS) of (53) is upper bounded by \f\f\fTπt+1 I V t(s) − T ¯πt+1 I V t(s) \f\f\f ≤ ¯εθ p |A|Rmax 1 − γ . (54) To finish the proof, it suffices to bound the second term on the RHS of (53). By the definition of the Bellman operators, we have \r\r\rT ˜πt+1 I V t − TIV t \r\r\r = \r\r\r\r\r X m∈C q′ m NX n=1 qn \u0010 Tπt+1 m I V t − TnV t \u0011\r\r\r\r\r ≤ X m∈C q′ m NX n=1 qn \r\r\rTπt+1 m I V t − TnV t \r\r\r. (55) The RHS of (55) can be further bounded by \r\r\rTπt+1 m I V t − TnV t \r\r\r ≤ \r\r\rTπt+1 m I V t − Tπt+1 m m V t \r\r\r + \r\r\rTπt+1 m m V t − TmV t \r\r\r + \r\rTmV t − TnV t\r\r ≤ γRmaxκm,I 1 − γ + ϵm + γRmaxκm,n 1 − γ , (56) 32Client Selection for Federated Policy Optimization where the last inequality follows from Lemma 15 and (7). By substituting (56) into (55), we can obtain \r\r\rTπt+1 I V πt − TIV πt \r\r\r ≤ X m∈C NX n=1 q′ mqn \r\r\rTπt+1 m I V πt − TnV πt \r\r\r ≤ X m∈C NX n=1 q′ mqn γRmaxκm,n 1 − γ + X m∈C q′ m γRmaxκm,I 1 − γ + X m∈C q′ mϵm. (57) By substituting (57) into (53), we can conclude \r\r\rTπt+1 I V t − TIV t \r\r\r ≤ ˜εθ p |A|Rmax 1 − γ + X m∈C NX n=1 q′ mqn γRmaxκm,n 1 − γ + X m∈C q′ m γRmaxκm,I 1 − γ + X m∈C q′ mϵm. Appendix F. Proof of Proposition 14 Proof By Theorem 10, we have lim sup t→∞ \f\f\f¯V πt (s) − ¯V max s \f\f\f ≤ ˜ϵ + 2γ ˙δ (1 − γ)2 + 2γRmaxκ1 (1 − γ)2 , where ˜ϵ may be one of´ϵ (Proposition 9) or ϵ′ (Lemma 5). Since the environments are homogeneous, we have ¯V max s = ¯V π∗ (s) = ¯V π∗ I (s), κ1 = κ2 = 0, ˙δ = ¯δ, ϵ′ = ´ϵ = ¯ϵ + ˆεθ p |A|Rmax 1 − γ , where ˆεθ is equal to¯εθ and ˜εθ for full participation and partial participation, respectively. Thus, we can conclude lim sup t→∞ \r\r\r¯V πt − ¯V π∗ \r\r\r ≤ ˆεθ p |A|Rmax (1 − γ)3 + 2γ¯εw (1 − γ)2 + ¯ϵ + 2γ¯δ (1 − γ)2 . 33Xie and Song Appendix G. Proof of Lemma 12 Our proof relies on the definition of local linearization for the two-layer neural network at its random initialization, which was first introduced by (Cai et al., 2019; Wang et al., 2019; Liu et al., 2019): u0 wt(s) = 1√m mX i bi · 1 n\u0000 w0 i \u0001T (s) > 0 o\u0000 wt i \u0001T (s), f0 θt(s, a) = 1√m mX i bi · 1 n\u0000 θ0 i \u0001T (s, a) > 0 o\u0000 θt i \u0001T (s, a). The following lemma characterizes the error induced by the above local linearization. Lemma 18 For w ∈ B0 Rw , θ∈ B0 Rθ , s∈ S, anda ∈ A, we have Einit \u0002\f\ffθ(s, a) − f0 θ (s, a) \f\f\u0003 = O \u0010 R6/5 θ m−1/10 ˆR2/5 θ \u0011 , (58) Einit \u0002\f\fuw(s) − u0 w(s) \f\f\u0003 = O \u0010 R6/5 w m−1/10 ˆR2/5 w \u0011 . (59) Proof Given any pair of model parametersθ ∈ B0 Rθ and θ′ ∈ B0 Rθ , 1 \b θT i (s, a) > 0 \t ̸= 1 n\u0000 θ′ i \u0001T (s, a) > 0 o implies \f\f\f \u0000 θ′ i \u0001T (s, a) \f\f\f ≤ \f\f\fθT i (s, a) − \u0000 θ′ i \u0001T (s, a) \f\f\f ≤ \r\rθi − θ′ i \r\r 2 . Consequently, we have \f\ffθ(s, a) − f0 θ (s, a) \f\f = 1√m \f\f\f\f\f mX i=1 bi · \u0010 1 \b θT i (s, a) > 0 \t − 1 n\u0000 θ0 i \u0001T (s, a) > 0 o\u0011 · θT i (s, a) \f\f\f\f\f ≤ 1√m mX i=1 \f\f\f1 \b θT i (s, a) > 0 \t − 1 n\u0000 θ0 i \u0001T (s, a) > 0 o\f\f\f · \f\fθT i (s, a) \f\f ≤ 1√m mX i=1 1 n\u0000 θ0 i \u0001T (s, a) ≤ \r\rθi − θ0 i \r\r 2 o · \r\rθi − θ0 i \r\r 2 . (60) Next, we analyzeEinit \u0002\f\ffθ(s, a) − f0 θ (s, a) \f\f\u0003 by examining two cases. • Case 1: G1 = 1√m P i∈C1 \u0010 1 n\u0000 θ0 i \u0001T (s, a) ≤ \r\rθi − θ0 i \r\r 2 o\u0011 · \r\rθi − θ0 i \r\r 2, where C1 =\b i ∈ [m] : \r\rθi − θ0 i \r\r 2 ≤ ∆ \t and ∆ > 0. Without loss of generality, we assume that parametersθ(0) are uniformly initialized from a circle with a radius ofR0. Then, the number of neurons lying inC1 is approximately m∆ R0 . Thus, we have G1 ≤ 1√m X i∈C1 \r\rθi − θ0 i \r\r 2 = O \u0010 m1/2∆2R−1 0 \u0011 . (61) 34Client Selection for Federated Policy Optimization Note that the size ofC1 decreases as∆ decreases. Given a fixedm, a sufficiently small ∆ exists that makes (61) negligible. • Case 2: G2 = 1√m P i∈C2 \u0010 1 n\u0000 θ0 i \u0001T (s, a) ≤ \r\rθi − θ0 i \r\r 2 o\u0011 · \r\rθi − θ0 i \r\r 2, where C2 =\b i ∈ [m] : \r\rθi − θ0 i \r\r 2 > ∆ \t and ∆ > 0. We have \r\rθi − θ0 i \r\r 2 / \r\rθ0 i \r\r 2 > ∆/ ˆRθ. Consequently, there exits a constantc ≥ ˆRθ/∆ such that for any layeri ∈ C2, a∈ A, s∈ S, it holds that 1 n\u0000 θ0 i \u0001T (s, a) ≤ \r\rθi − θ0 i \r\r 2 o ≤ 1 ≤ c \r\rθi − θ0 i \r\r 2 / \r\rθ0 i \r\r 2 . (62) Next, by the Cauchy-Schwarz inequality and \r\rθ − θ0\r\r 2 ≤ Rθ, we have G2 ≤ Rθ√m sX i∈C2 1 n\u0000 θ0 i \u0001T (s, a) ≤ \r\rθi − θ0 i \r\r 2 o . (63) By (62) and taking expectation on both sides of (63), we can obtain Einit [G2] ≤ Rθ√mEinit   sX i∈C2 1 n\u0000 θ0 i \u0001T (s, a) ≤ \r\rθi − θ0 i \r\r 2 o   ≤ Rθ√m vuuutEinit  X i∈C2 1 n\u0000 θ0 i \u0001T (s, a) ≤ \r\rθi − θ0 i \r\r 2 o   ≤ Rθ√m vuuutcEinit  X i∈C2 \r\rθi − θ0 i \r\r 2 /∥θ0 i ∥2  . By the Cauchy-Schwarz inequality, we have Einit  X i∈C2 \r\rθi − θ0 i \r\r 2 /∥θ0 i ∥2   ≤ Einit  X i∈C2 \r\rθi − θ0 i \r\r2 2   1/2 · Einit  X i∈C2 ∥θ0 i ∥−2 2   1/2 ≤ Rθ · Einit  X i∈C2 ∥θ0 i ∥−2 2   1/2 , (64) where the second inequality follows fromPm i=1 \r\rθi − θ0 i \r\r2 2 = \r\rθ − θ0\r\r2 2 ≤ R2 θ. Since Einit h ∥θi∥−2 2 i ≤ ∞, ∀i ∈ [m] by the initialization scheme (16), we have that the RHS of (64) isO(Rθm1/2). Thus, we can obtain Einit [G2] = O \u0010 R3/2 θ m−1/4 ˆR1/2 θ ∆−1/2 \u0011 . (65) 35Xie and Song By (61) and (65), we can obtain Einit \u0002\f\ffθ(s, a) − f0 θ (s, a) \f\f\u0003 ≤ Einit [G1 + G2] = O \u0010 R3/2 θ m−1/4 ˆR1/2 θ ∆−1/2 + m1/2∆2R−1 0 \u0011 . We further assumem1/2∆2R−1 0 ≤ ϱ, i.e.,∆ ≤ ϱ1/2m−1/4R1/2 0 , which implies that Einit \u0002\f\ffθ(s, a) − f0 θ (s, a) \f\f\u0003 = O \u0010 R3/2 θ m−1/4 ˆR1/2 θ ∆−1/2 + ϱ \u0011 = O \u0010 R3/2 θ m−1/8 ˆR1/2 θ ϱ−1/4R−1/4 0 + ϱ \u0011 . Moreover, we assume thatR3/2 θ m−1/8 ˆR1/2 θ ϱ−1/4R−1/4 0 ≥ ϱ, i.e.,ϱ ≤ R6/5 θ m−1/10 ˆR2/5 θ R−1/5 0 , which gives Einit \u0002\f\ffθ(s, a) − f0 θ (s, a) \f\f\u0003 = O \u0010 R6/5 θ m−1/10 ˆR2/5 θ \u0011 . This completes the proof of (58), and the proof of (59) is similar. The following lemma provides the upper bound of the difference between network outputs. Lemma 19 For states ∈ S, any pair of actionsa and a′, and model parametersϑ, ϑ′ ∈ B0 Rϑ, which isθ for the policy andw for the value function, we have Einit \u0002\f\fuϑ(s, a) − uϑ′(s, a′) \f\f\u0003 = O (Rϑ) , (66) Einit [|uϑ(s, a) − uϑ′(s, a)|] = O (Rϑ) . (67) Proof By Jensen’s inequality, we have Einit \f\f\u0002 uϑ(s, a) − uϑ′(s, a′) \f\f\u00032 ≤ 1 mEinit   \f\f\f\f\f mX i bi · 1 \b ϑT i (s, a) > 0 \t ϑT i (s, a) − mX i bi · 1 n\u0000 ϑ′ i \u0001T (s, a′) > 0 o\u0000 ϑ′ i \u0001T (s, a′) \f\f\f\f\f 2 . By the fact that(a + b)2 ≤ 2a2 + 2b2 and ab − cd = a(b − d) + d(a − c), we have Einit \f\f\u0002 uϑ(s, a) − uϑ′(s, a′) \f\f\u00032 ≤ 1 mEinit \"\f\f\f\f\f mX i bi · 1 \b ϑT i (s, a) > 0 \t\u0010 ϑT i (s, a) − \u0000 ϑ′ i \u0001T (s, a′) \u0011 + mX i bi · \u0010 1 n\u0000 ϑ′ i \u0001T (s, a′) > 0 o − 1 n\u0000 ϑ′ i \u0001T (s, a′) > 0 o\u0011\u0000 ϑ′ i \u0001T (s, a′) \f\f\f\f\f 2  ≤ 1 mEinit  2 \f\f\f\f\f mX i bi · 1 \b ϑT i (s, a) > 0 \t\u0010 ϑT i (s, a) − \u0000 ϑ′ i \u0001T (s, a′) \u0011\f\f\f\f\f 2 +2 \f\f\f\f\f mX i bi · \u0010 1 n\u0000 ϑ′ i \u0001T (s, a′) > 0 o − 1 n\u0000 ϑ′ i \u0001T (s, a′) > 0 o\u0011\u0000 ϑ′ i \u0001T (s, a′) \f\f\f\f\f 2 . 36Client Selection for Federated Policy Optimization Furthermore, we have ∥ϑ∥2 2 ≤ \u0000\r\rϑ − ϑ0\r\r 2 + \r\rϑ0\r\r 2 \u00012 ≤ 2R2 ϑ + 2 \r\rϑ0\r\r2 2 . (68) By (68) and the Cauchy-Schwarz inequality, we have Einit \f\f\u0002 uϑ(s, a) − uϑ′(s, a′) \f\f\u00032 ≤ 2Einit \" mX i \u0010 ϑT i (s, a) − \u0000 ϑ′ i \u0001T (s, a′) \u00112 ! + \r\rϑ′\r\r2 2 # ≤ 2Einit \" mX i 2 \u0000 ϑT i (s, a) \u00012 + 2 \u0010\u0000 ϑ′ i \u0001T (s, a′) \u00112 ! + \r\rϑ′\r\r2 2 # ≤ 6Einit h\r\rϑ′\r\r2 2 i + 4Einit h ∥ϑ∥2 2 i ≤ 20R2 ϑ + 20. (69) We can then complete the proof of (66) by taking the square root of both sides of (69). The proof for (67) is similar. Next, we prove Lemma 12. Proof Let (t′, s′) = arg maxt>0,s∈S \f\fV t(s) − ¯V t(s) \f\f, which are dependent on the initialization w0. By the triangle inequality and Lemma 18, we have Einit [¯εw] = Einit h max t \r\rV t − ¯V t\r\r i ≤ Einit h\f\f\fV t′ (s′) − V t′,0(s′) \f\f\f + \f\f\fV t′,0(s′) − ¯V t′ (s′) \f\f\f i = Einit \"\f\f\fV t′ (s′) − V t′,0(s′) \f\f\f + \f\f\f\f\f NX n=1 qnV t′,0 n (s′) − NX n=1 qnV t′ n (s′) \f\f\f\f\f # = O \u0010 R6/5 w m−1/10 ˆR2/5 w \u0011 , which completes the proof of (18). Since the order of terms in the square \u0010 πt(a|s) − PN n=1 qnπt n(a|s) \u00112 does not affect the value, we define two setsC1 = n t >0, a∈ A, s∈ S: πt(a|s) > PN n=1 qnπt n(a|s) o , andC2 = CC 1 . Accordingly, forall(t, a, s) ∈ C1, wehaveπt(a|s)−PN n=1 qnπt n(a|s) ≤ πt(a|s) log πt(a|s)PN n=1 qnπtn(a|s) by the fact that1 − 1 x ≤ ln x, ∀x >0. By the Arithmetic Mean-Geometric Mean (AM-GM) inequality, we have   πt(a|s) − NX n=1 qnπt n(a|s) !2 ≤   πt(a|s) − NX n=1 qnπt n(a|s) ! πt(a|s)   log πt(a|s)PN n=1 qnπtn(a|s) ! ≤   πt(a|s) − NX n=1 qnπt n(a|s) ! πt(a|s)  NX n=1 qn log πt(a|s) πtn(a|s) ! ≤ πt(a|s) NX n=1 qn   fθt(s, a) − fθtn(s, a) + log P a′∈A exp \u0000 fθtn(s, a′) \u0001 P a′∈A exp (fθt(s, a′)) ! . 37Xie and Song For all(t, a, s) ∈ C2, we havePN n=1 qnπt n(a|s) − πt(a|s) = PN n=1 qn \u0000 πt n(a|s) − πt(a|s) \u0001 ≤PN n=1 qnπt n(a|s) log πt n(a|s) πt(a|s) by the fact that1 − 1 x ≤ ln x, ∀x >0. Therefore, we have   πt(a|s) − NX n=1 qnπt n(a|s) !2 ≤ NX n=1 qnπt n(a|s) log πt n(a|s) πt(a|s) . Let (t′, s′) = arg maxt>0,s∈S \r\r\rπt(·|s) − PN n=1 qnπt n(·|s) \r\r\r 2 , which is conditional onθ0, we have ¯εθ = \r\r\r\r\rπt′ (·|s′) − NX n=1 qnπt′ n (·|s′) \r\r\r\r\r 2 = vuutX a∈A   πt′ (a|s′) − NX n=1 qnπt′ n (a|s′) !2 . Let A1 = {a′ ∈ A: t′, a′, s′ ∈ C1} and A2 = {a′ ∈ A: t′, a′, s′ ∈ C2}, we have ¯εθ ≤   X a∈A1 NX n=1 qnπt′ (a|s′)   fθt′ (s′, a) − fθt′ n (s′, a) + log P a′∈A exp \u0000 fθtn(s′, a′) \u0001 P a′∈A exp (fθt(s′, a′)) ! + X a∈A2 NX n=1 qnπt′ n (a|s′)   fθt′ n (s′, a) − fθt′ (s′, a) + log P a′∈A exp (fθt(s′, a′))P a′∈A exp \u0000 fθtn(s′, a′) \u0001 !  1/2 ≤  X a∈A NX n=1 qn max n πt′ n (a|s′), πt′ (a|s′) o\u0010\f\f\ffθt′ (s′, a) − fθt′ n (s′, a) \f\f\f + \f\f\f\f\flog P a′∈A exp (fθt(s′, a′))P a′∈A exp \u0000 fθtn(s′, a′) \u0001 \f\f\f\f\f !!1/2 . By taking the maximum overa ∈ A, we have ¯εθ ≤   max a∈A,n∈[N]  \f\f\ffθt′ (s′, a) − fθt′ n (s′, a) \f\f\f + \f\f\f\f\flog P a′∈A exp (fθt(s′, a′))P a′∈A exp \u0000 fθtn(s′, a′) \u0001 \f\f\f\f\f ! · X a∈A NX n=1 qn max n πt′ n (a|s′), πt′ (a|s′) o!1/2 ≤   2 max a∈A,n∈[N]  \f\f\ffθt′ (s′, a) − fθt′ n (s′, a) \f\f\f + \f\f\f\f\flog P a′∈A exp (fθt(s′, a′))P a′∈A exp \u0000 fθtn(s′, a′) \u0001 \f\f\f\f\f !!1/2 . By Jensen’s inequality, we have Einit [¯εθ] ≤ Einit \" 2 max a,n \f\f\ffθt′ (s′, a) − fθt′ n (s′, a) \f\f\f + 2 \f\f\f\f\flog P a′∈A exp (fθt(s′, a′))P a′∈A exp \u0000 fθtn(s′, a′) \u0001 \f\f\f\f\f #1/2 . (70) It remains to bound the two absolute terms on the RHS of (70). The first absolute term can be bounded by Lemma 19 as Einit \u0014 max a∈A,n∈[N] \f\f\ffθt′ (s′, a) − fθt′ n (s′, a) \f\f\f \u0015 = O (Rθ) . (71) 38Client Selection for Federated Policy Optimization Next, we bound the second absolute term on the RHS of (70). By the log-sum inequality, the log-sum-exp trick, and Lemma 19, we can obtain log X a′∈A exp \u0000 fθt(s, a′) \u0001 − log X a′∈A exp \u0000 fθtn(s, a′) \u0001 ≤ max a∈A fθt(s, a) − 1 |A| X a′ \u0000 fθtn(s, a′) \u0001 , and log X a′∈A exp \u0000 fθtn(s, a′) \u0001 − log X a′∈A exp \u0000 fθt(s, a′) \u0001 ≤ max a∈A fθtn(s, a) − 1 |A| X a′ \u0000 fθt(s, a′) \u0001 , which indicates that Einit \" max a∈A,n∈[N] \f\f\f\f\flog P a′∈A exp (fθt(s, a′))P a′∈A exp \u0000 fθtn(s, a′) \u0001 \f\f\f\f\f # = O (Rθ) . (72) By substituting (71) and (72) into (70), we can obtain Einit [¯εθ] = O \u0010 R1/2 θ \u0011 , which completes the proof of (19). The proof of (20) is similar. Appendix H. Additional Experiment Setting Machines: We simulate the federated learning experiments (1 server and N devices) on a commodity machine with 16 Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHZz. It took about 6 mins to finish one round of training, i.e., 50 hours to obtain 500 data points for Figure 2. The hyperparameters for the algorithms on MountainCars, Hoppers, HongKongOSMs, and the general FRL setting are given in Table 1, Table 2, Table 3, and Table 4, respectively. Hyperparameter FedPOHCS FedAvg Power-of-Choice GradientNorm Learning Rate 0.001 0.005 0.001 0.001 Learning Rate Decay 0.98 0.98 0.98 0.98 Batch Size 128 128 128 128 Timestep per Iteration 2048 2048 2048 2048 Number of Epochs (E) 1 1 1 1 Discount Factor (γ) 0.99 0.99 0.99 0.99 Discount Factor for GAE 0.95 0.95 0.95 0.95 KL Target 0.003 0.003 0.003 0.003 Table 1: Hyperparameters for each algorithm on MountainCars. 39Xie and Song Hyperparameter FedPOHCS FedAvg Power-of-Choice GradientNorm Learning Rate 0.03 0.03 0.03 0.03 Learning Rate Decay 0.9 0.9 0.9 0.9 Batch Size 128 128 128 128 Timestep per Iteration 2048 2048 2048 2048 Number of Epochs (E) 1 1 1 1 Discount Factor (γ) 0.99 0.99 0.99 0.99 Discount Factor for GAE 0.95 0.95 0.95 0.95 KL Target 0.003 0.003 0.003 0.003 Table 2: Hyperparameters for each algorithm on Hoppers. Hyperparameter FedPOHCS FedAvg Power-of-Choice GradientNorm Learning Rate 0.0001 0.0001 0.0001 0.0001 Learning Rate Decay 0.98 0.98 0.98 0.98 Batch Size 128 128 128 128 Timestep per Iteration 2048 2048 2048 2048 Number of Epochs (E) 10 10 10 10 Discount Factor (γ) 0.99 0.99 0.99 0.99 Discount Factor for GAE 0.95 0.95 0.95 0.95 KL Target 0.0001 0.0001 0.0001 0.0001 Table 3: Hyperparameters for each algorithm on HongKongOSMs. Environment #Client (N) #Candidate (d) #Participant (K) #Local Iteration (I) MountainCars 60 18 6 5 Hoppers 60 18 6 20 HongKongOSMs 10 9 2 10 Table 4: General FRL Setting. Refer to Section 2.1 and Algorithm 3 for their definitions. References Sanjeev Arora, Simon Du, Wei Hu, Zhiyuan Li, and Ruosong Wang. Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors,Proceedings of the 36th Interna- tional Conference on Machine Learning, volume 97 ofProceedings of Machine Learning Research, pages 322–332. PMLR, 09–15 Jun 2019. Dimitri Bertsekas.Abstract dynamic programming. Athena Scientific, 2022. Dimitri Bertsekas and John N Tsitsiklis.Neuro-dynamic programming. Athena Scientific, 1996. Dimitri P. Bertsekas. Approximate policy iteration: a survey and some new methods. Journal of Control Theory and Applications, 9(3):310–335, Aug 2011. ISSN 1993-0623. doi: 40Client Selection for Federated Policy Optimization 10.1007/s11768-011-1005-3. Huang Bojun. Steady state analysis of episodic reinforcement learning. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors,Advances in Neural Information Processing Systems, volume 33, pages 9335–9345. Curran Associates, Inc., 2020. Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and Wojciech Zaremba. Openai gym.CoRR, abs/1606.01540, 2016. Qi Cai, Zhuoran Yang, Jason D Lee, and Zhaoran Wang. Neural temporal-difference learning converges to global optima. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Mingzhe Chen, H. Vincent Poor, Walid Saad, and Shuguang Cui. Convergence time opti- mization for federated learning over wireless networks.IEEE Transactions on Wireless Communications, 20(4):2457–2471, 2021. doi: 10.1109/TWC.2020.3042530. Minmin Chen, Alex Beutel, Paul Covington, Sagar Jain, Francois Belletti, and Ed H. Chi. Top-k off-policy correction for a reinforce recommender system. InProceedings of the Twelfth ACM International Conference on Web Search and Data Mining, WSDM ’19, page 456–464, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450359405. doi: 10.1145/3289600.3290999. Wenlin Chen, Samuel Horváth, and Peter Richtárik. Optimal client sampling for federated learning. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. Kamil Ciosek and Shimon Whiteson. Expected policy gradients for reinforcement learning. Journal of Machine Learning Research, 21(52):1–51, 2020. Rémi Coulom. Reinforcement learning using neural networks, with applications to motor control. PhD thesis, Institut National Polytechnique de Grenoble-INPG, 2002. Xiaofeng Fan, Yining Ma, Zhongxiang Dai, Wei Jing, Cheston Tan, and Bryan Kian Hsiang Low. Fault-tolerant federated reinforcement learning with theoretical guarantee.Advances in Neural Information Processing Systems, 34, 2021. Yae Jee Cho, Jianyu Wang, and Gauri Joshi. Towards understanding biased client selection in federated learning. In Gustau Camps-Valls, Francisco J. R. Ruiz, and Isabel Valera, editors, Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 ofProceedings of Machine Learning Research, pages 10351–10375. PMLR, 28–30 Mar 2022. Xiaoqian Jiang, Miran Kim, Kristin Lauter, and Yongsoo Song. Secure outsourced matrix computation and application to neural networks. InProceedings of the 2018 ACM SIGSAC conference on computer and communications security, pages 1209–1222, 2018. HaoJin, YangPeng, WenhaoYang, ShusenWang, andZhihuaZhang. Federatedreinforcement learning with environment heterogeneity. In Gustau Camps-Valls, Francisco J. R. Ruiz, 41Xie and Song and Isabel Valera, editors,Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 ofProceedings of Machine Learning Research, pages 18–37. PMLR, 28–30 Mar 2022. Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Proceedings of the Nineteenth International Conference on Machine Learning, pages 267–274, 2002. ISBN 1558608737. Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia Smith. Fair resource allocation in federated learning. In8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. InICLR 2020 : Eighth International Conference on Learning Representations, 2020. Yijing Li, Xiaofeng Tao, Xuefei Zhang, Junjie Liu, and Jin Xu. Privacy-preserved federated learning for autonomous driving.IEEE Transactions on Intelligent Transportation Systems, 23(7):8423–8434, 2022. doi: 10.1109/TITS.2021.3081560. Xinle Liang, Yang Liu, Tianjian Chen, Ming Liu, and Qiang Yang. Federated transfer reinforcement learning for autonomous driving.CoRR, abs/1910.06001, 2019. Hyun-Kyo Lim, Ju-Bong Kim, Joo-Seong Heo, and Youn-Hee Han.Federated Reinforcement Learning for Training Control Policies on Multiple IoT Devices, volume 20. Sensors, 2020. doi: 10.3390/s20051359. Boyi Liu, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural trust region/proximal policy optimization attains globally optimal policy. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Ouiame Marnissi, Hajar El Hammouti, and El Houcine Bergou. Client selection in federated learning based on gradients importance, 2021. Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-Efficient Learning of Deep Networks from Decentralized Data. In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, volume 54, pages 1273–1282, 20–22 Apr 2017. Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In Maria Florina Balcan and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine Learning, volume 48, pages 1928–1937. PMLR, 20–22 Jun 2016. Thomas M. Moerland, Joost Broekens, and Catholijn M. Jonker. Model-based reinforcement learning: A survey.CoRR, abs/2006.16712, 2020. 42Client Selection for Federated Policy Optimization Andrew William Moore. Efficient memory-based learning for robot control. Technical report, University of Cambridge, 1990. Seongin Na, Tomáš Rouček, Jiří Ulrich, Jan Pikman, Tom s Krajník, Barry Lennox, and Farshad Arvin. Federated reinforcement learning for collective navigation of robotic swarms. IEEE Transactions on Cognitive and Developmental Systems, pages 1–1, 2023. doi: 10.1109/TCDS.2023.3239815. Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, Shane Legg, Volodymyr Mnih, Koray Kavukcuoglu, and David Silver. Massively parallel methods for deep reinforcement learning.CoRR, abs/1507.04296, 2015. OpenStreetMap contributors. Planet dump retrieved from https://planet.osm.org .https: //www.openstreetmap.org, 2017. Melkior Ornik and Ufuk Topcu. Learning and planning for time-varying mdps using maximum likelihood estimation. The Journal of Machine Learning Research, 22(1):1656–1695, 2021. Matteo Papini, Damiano Binaghi, Giuseppe Canonaco, Matteo Pirotta, and Marcello Restelli. Stochastic variance-reduced policy gradient. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 4026–4035. PMLR, 10–15 Jul 2018. Matteo Pirotta, Marcello Restelli, Alessio Pecorino, and Daniele Calandriello. Safe policy iteration. In Sanjoy Dasgupta and David McAllester, editors,Proceedings of the 30th International Conference on Machine Learning, volume 28 ofProceedings of Machine Learning Research, pages 307–315, Atlanta, Georgia, USA, 17–19 Jun 2013. PMLR. Jiaju Qi, Qihao Zhou, Lei Lei, and Kan Zheng. Federated reinforcement learning: Techniques, applications, and open challenges.CoRR, abs/2108.11887, 2021. John Schulman, Sergey Levine, Philipp Moritz, Michael I. Jordan, and Pieter Abbeel. Trust region policy optimization.CoRR, abs/1502.05477, 2015. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High- dimensional continuous control using generalized advantage estimation. InProceedings of the International Conference on Learning Representations (ICLR), 2016. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms.CoRR, abs/1707.06347, 2017. David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George van den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of go with deep neural networks and tree search.Nature, 529(7587): 484–489, Jan 2016. ISSN 1476-4687. doi: 10.1038/nature16961. 43Xie and Song Lauren N Steimle, David L Kaufman, and Brian T Denton. Multi-model markov decision processes. IISE Transactions, 53(10):1124–1139, 2021. Alexander L. Strehl and Michael L. Littman. An analysis of model-based interval estimation for markov decision processes.Journal of Computer and System Sciences, 74(8):1309–1331, 2008. ISSN 0022-0000. doi: https://doi.org/10.1016/j.jcss.2007.08.009. Learning Theory 2005. Alexander L. Strehl, Lihong Li, and Michael L. Littman. Reinforcement learning in finite mdps: Pac analysis.Journal of Machine Learning Research, 10(84):2413–2444, 2009. Richard S. Sutton and Andrew G. Barto.Reinforcement Learning: An Introduction. The MIT Press, second edition, 2018. John E. Vargas-Munoz, Shivangi Srivastava, Devis Tuia, and Alexandre X. Falcão. Open- streetmap: Challenges and opportunities in machine learning and remote sensing.IEEE Geoscience and Remote Sensing Magazine, 9(1):184–199, 2021. doi: 10.1109/MGRS.2020. 2994107. Nino Vieillard, Olivier Pietquin, and Matthieu Geist. Deep conservative policy iteration. Proceedings of the AAAI Conference on Artificial Intelligence, 34(04):6070–6077, Apr. 2020. doi: 10.1609/aaai.v34i04.6070. Eugene Vinitsky, Aboudy Kreidieh, Luc Le Flem, Nishant Kheterpal, Kathy Jang, Cathy Wu, Fangyu Wu, Richard Liaw, Eric Liang, and Alexandre M. Bayen. Benchmarks for reinforcement learning in mixed-autonomy traffic. InProceedings of The 2nd Conference on Robot Learning, volume 87, pages 399–409, 2018. Paul Wagner. A reinterpretation of the policy oscillation phenomenon in approximate policy iteration. In J. Shawe-Taylor, R. Zemel, P. Bartlett, F. Pereira, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 24. Curran Associates, Inc., 2011. Lingxiao Wang, Qi Cai, Zhuoran Yang, and Zhaoran Wang. Neural policy gradient methods: Global optimality and rates of convergence.arXiv preprint arXiv:1909.01150, 2019. Yu Xianjia, Jorge Peña Queralta, Jukka Heikkonen, and Tomi Westerlund. Federated learning in robotic and autonomous systems.Procedia Computer Science, 191:135–142, 2021. ISSN 1877-0509. doi: https://doi.org/10.1016/j.procs.2021.07.041. The 18th International Conference on Mobile Systems and Pervasive Computing (MobiSPC), The 16th International Conference on Future Networks and Communications (FNC), The 11th International Conference on Sustainable Energy Information Technology. Zhijie Xie and Shenghui Song. Fedkl: Tackling data heterogeneity in federated reinforcement learning by penalizing kl divergence.IEEE Journal on Selected Areas in Communications, 41(4):1227–1242, 2023. doi: 10.1109/JSAC.2023.3242734. Chenyu Zhang, Han Wang, Aritra Mitra, and James Anderson. Finite-time analysis of on-policy heterogeneous federated reinforcement learning. InThe Twelfth International Conference on Learning Representations, 2024. 44Client Selection for Federated Policy Optimization Kaiqing Zhang, Zhuoran Yang, and Tamer Basar. Multi-agent reinforcement learning: A selective overview of theories and algorithms.CoRR, abs/1911.10635, 2019. Doudou Zhou, Yufeng Zhang, Aaron Sonabend-W, Zhaoran Wang, Junwei Lu, and Tianxi Cai. Federated offline reinforcement learning, 2022. Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement learning. InAaai, volume 8, pages 1433–1438. Chicago, IL, USA, 2008. 45",
      "references": [
        "Mastering the game of go with deep neural networks and tree search",
        "Top-k off-policy correction for a reinforce recommender system",
        "Expected policy gradients for reinforcement learning",
        "Fault-tolerant federated reinforcement learning with theoretical guarantee",
        "Stochastic variance-reduced policy gradient",
        "Multi-agent reinforcement learning: A selective overview of theories and algorithms",
        "Massively parallel methods for deep reinforcement learning",
        "Asynchronous methods for deep reinforcement learning",
        "Federated transfer reinforcement learning for autonomous driving",
        "Privacy-preserved federated learning for autonomous driving",
        "Federated learning in robotic and autonomous systems",
        "Federated reinforcement learning for collective navigation of robotic swarms",
        "Federated Reinforcement Learning for Training Control Policies on Multiple IoT Devices",
        "Federated offline reinforcement learning",
        "Federated reinforcement learning: Techniques, applications, and open challenges",
        "Fedkl: Tackling data heterogeneity in federated reinforcement learning by penalizing kl divergence",
        "Federated reinforcement learning with environment heterogeneity",
        "Finite-time analysis of on-policy heterogeneous federated reinforcement learning",
        "Safe policy iteration",
        "Trust region policy optimization",
        "Deep conservative policy iteration",
        "Abstract dynamic programming",
        "Neuro-dynamic programming",
        "Approximate policy iteration: a survey and some new methods",
        "A reinterpretation of the policy oscillation phenomenon in approximate policy iteration",
        "Fair resource allocation in federated learning",
        "On the convergence of fedavg on non-iid data",
        "Towards understanding biased client selection in federated learning",
        "Communication-Efficient Learning of Deep Networks from Decentralized Data",
        "Reinforcement Learning: An Introduction",
        "Multi-model markov decision processes",
        "Fine-grained analysis of optimization and generalization for overparameterized two-layer neural networks",
        "Neural temporal-difference learning converges to global optima",
        "Neural policy gradient methods: Global optimality and rates of convergence",
        "Neural trust region/proximal policy optimization attains globally optimal policy",
        "Steady state analysis of episodic reinforcement learning",
        "Approximately optimal approximate reinforcement learning",
        "Model-based reinforcement learning: A survey",
        "Reinforcement learning in finite mdps: Pac analysis",
        "Learning and planning for time-varying mdps using maximum likelihood estimation",
        "An analysis of model-based interval estimation for markov decision processes",
        "Maximum entropy inverse reinforcement learning",
        "High-dimensional continuous control using generalized advantage estimation",
        "Proximal policy optimization algorithms",
        "Secure outsourced matrix computation and application to neural networks",
        "Efficient memory-based learning for robot control",
        "Reinforcement learning using neural networks, with applications to motor control",
        "Benchmarks for reinforcement learning in mixed-autonomy traffic",
        "Open-streetmap: Challenges and opportunities in machine learning and remote sensing",
        "Openai gym",
        "Convergence time optimization for federated learning over wireless networks",
        "Client selection in federated learning based on gradients importance"
      ],
      "meta_data": {
        "arxiv_id": "2305.10978v7",
        "authors": [
          "Zhijie Xie",
          "Shenghui Song"
        ],
        "published_date": "2023-05-18T13:48:20Z",
        "github_url": ""
      },
      "llm_extracted_info": {
        "main_contributions": "The paper investigates Federated Approximate Policy Iteration (FAPI) under heterogeneous environments, deriving its error bound and explicitly revealing the impact of heterogeneity. It theoretically proves that a proper client selection scheme can reduce this error bound. Based on these theoretical results, a novel client selection algorithm, Federated Policy Optimization with Heterogeneity-aware Client Selection (FedPOHCS), is proposed to mitigate the additional approximation error caused by environment heterogeneity. The algorithm's efficacy is validated on federated mountain car, Mujoco Hopper, and SUMO-based autonomous vehicle training problems.",
        "methodology": "The core methodology involves a theoretical analysis of Federated Approximate Policy Iteration (FAPI) using an 'imaginary MDP' (average of all clients’ environments) to derive error bounds. The proposed FedPOHCS algorithm leverages a client selection metric (∆n) that combines the learning potential (magnitude of advantage function) and the level of environmental heterogeneity (deviation from the imaginary MDP). For implementation, tabular maximum likelihood models are used to estimate transition probabilities and reward functions. Generalized Advantage Estimation (GAE) is employed for advantage function estimation, and Proximal Policy Optimization (PPO) serves as the local learning algorithm for clients. The paper also analyzes aggregation error with two-layer ReLU-activated neural networks for value function and policy parameterization.",
        "experimental_setup": "The proposed FedPOHCS algorithm is evaluated against FedAvg (random selection), Power-of-Choice, and GradientNorm on three federated reinforcement learning environments: 1) Federated Mountain Car (60 clients, heterogeneity by action shift), 2) Mujoco Hopper (60 clients, heterogeneity by leg size), and 3) SUMO-based Autonomous Vehicle Training (10 traffic networks from Hong Kong OSMs, high heterogeneity). Policies are represented by Multilayer Perceptrons (MLPs) with tanh non-linearity and (64, 64) hidden layers. Experiments use the SGD optimizer with momentum 0.9 and varying learning rate decays. Performance is evaluated by the mean returns reported after clients interact with their MDPs (10, 100, and 20 episodes for Mountain Cars, Hoppers, and HongKongOSMs, respectively), averaged across three independent runs with confidence intervals. Both two-phase and communication-efficient one-phase communication schemes are tested.",
        "limitations": "The proposed FedPOHCS implementation has several limitations: 1) It utilizes a two-phase communication scheme which is not communication-efficient, requiring an extra round for metric computation, though a less accurate one-phase variant exists. 2) It requires clients to upload state visitation frequencies and advantage functions, which may contain private information, necessitating privacy protection methods like Homomorphic Encryption. 3) The exact theoretical heterogeneity metric is hard to compute in practice, leading to approximations that might suffer from high variance. 4) The analysis of the error bound relies on an 'imaginary MDP', and bounding the distance between the actual optimal policy and the imaginary MDP's optimal policy is not always straightforward for all states.",
        "future_research_directions": "Future research directions include deeper exploration of client selection for Federated Reinforcement Learning (FRL), particularly focusing on developing more communication-efficient variants of client selection schemes that can use outdated information to compute metrics without significant performance degradation. Another promising area is the integration of privacy-preserving techniques, such as Homomorphic Encryption, to address the privacy concerns associated with clients uploading sensitive local information like state visitation frequencies and advantage functions. Further investigation into the trade-offs between non-linear function approximation (for training) and linear function approximation (for minimizing aggregation error) in neural network parameterization for FRL is also suggested.",
        "experimental_code": "",
        "experimental_info": ""
      }
    }
  ],
  "research_hypothesis": {
    "open_problems": "Current Chain-of-Thought (CoT) prompting often suffers from a fundamental limitation: its linear, unidirectional nature. While it decomposes problems, it lacks an explicit, structured mechanism for internal meta-analysis, critical self-assessment, and systematic error detection *within the same reasoning pass*. This often leads to models committing to initial flawed reasoning paths without the capacity to robustly critique their own logic, identify overlooked details, or systematically refine their solutions, particularly in complex, multi-step problem-solving scenarios.",
    "method": "We propose \"Structured Reflective Chain-of-Thought\" (SRCoT), an advanced prompt engineering technique that embeds a two-phase, meta-cognitive self-review process within a single, extended prompt. Building upon standard CoT, SRCoT first elicits an initial step-by-step reasoning path. Subsequently, it introduces a distinct \"Critique\" phase, explicitly instructing the model to analyze its *own generated initial reasoning* for potential logical inconsistencies, faulty assumptions, or missed constraints. Following this critique, a \"Refinement\" phase prompts the model to incorporate the insights from its self-critique to construct a revised, more robust reasoning path and a final, corrected answer. The prompt structure would be: `[Problem] + \"Let's think step by step.\" + [Initial CoT Reasoning] + \"Now, critically review your preceding reasoning. Identify any potential logical flaws, incorrect assumptions, or overlooked details. Provide a concise critique of your initial thought process.\" + [Critique of Initial Reasoning] + \"Based on your critique, re-evaluate the problem and provide a refined step-by-step reasoning and a corrected final answer.\" + [Refined CoT Reasoning and Answer]`. This method mimics human expert problem-solving, where an initial attempt is followed by critical self-evaluation and subsequent correction, without altering the model's architecture or requiring external tools.",
    "experimental_setup": "We will evaluate SRCoT against standard Chain-of-Thought (CoT) on challenging reasoning benchmarks. Specifically, we will use GSM8K for arithmetic reasoning and a subset of the Big-Bench Hard tasks (e.g., Word Problems, Date Understanding) that require multi-step logical deduction and careful attention to constraints. We will utilize a powerful instruction-tuned large language model (e.g., `text-bison@002` or `gpt-3.5-turbo`) in a zero-shot or few-shot setting. For each problem, responses will be generated using two strategies: 1) Standard CoT with \"Let's think step by step.\" and 2) SRCoT as described above. All experiments will maintain consistent model parameters (temperature, top-k, etc.) to ensure a fair comparison. A sufficiently large test set from each benchmark will be used to ensure statistical significance, with results averaged over multiple runs.",
    "primary_metric": "Accuracy",
    "experimental_code": "def generate_structured_reflective_cot_prompt(problem_statement: str) -> str:\n    \"\"\"\n    Generates a prompt for Structured Reflective Chain-of-Thought (SRCoT).\n    This prompt encourages the model to first think step-by-step,\n    then critically review its own initial reasoning, and finally\n    refine its solution based on the critique.\n    \"\"\"\n    initial_cot_instruction = \"Let's think step by step.\"\n    \n    critique_instruction = (\n        \"Now, critically review your preceding reasoning. \"\n        \"Identify any potential logical flaws, incorrect assumptions, or overlooked details. \"\n        \"Provide a concise critique of your initial thought process.\"\n    )\n    \n    refinement_instruction = (\n        \"Based on your critique, re-evaluate the problem and provide a refined \"\n        \"step-by-step reasoning and a corrected final answer.\"\n    )\n    \n    # The full prompt sent to the LLM would look like this,\n    # expecting the LLM to fill in the bracketed sections.\n    return (\n        f\"Problem: {problem_statement}\\n\\n\"\n        f\"{initial_cot_instruction}\\n\\n\"\n        f\"[Initial CoT Reasoning]\\n\\n\"\n        f\"{critique_instruction}\\n\\n\"\n        f\"[Critique of Initial Reasoning]\\n\\n\"\n        f\"{refinement_instruction}\\n\\n\"\n        f\"[Refined CoT Reasoning and Answer]\"\n    )\n",
    "expected_result": "We expect Structured Reflective Chain-of-Thought (SRCoT) to yield a significant accuracy improvement of 3-7% over standard Chain-of-Thought (CoT) on both GSM8K and Big-Bench Hard reasoning tasks. Specifically, on GSM8K, SRCoT is predicted to achieve an accuracy of 76-80%, surpassing CoT's typical 70-73%. On Big-Bench Hard tasks requiring multi-step logical deduction, SRCoT is expected to show an even greater relative improvement, reaching 45-50% accuracy compared to CoT's 38-42%. This enhanced performance will be particularly evident in problems where subtle errors, logical inconsistencies, or crucial details are often missed by linear reasoning. Higher accuracy values indicate better performance.",
    "expected_conclusion": "This research demonstrates that by introducing a structured, two-phase meta-cognitive self-review loop—comprising explicit critique and subsequent refinement—within a single prompt, large language models can significantly enhance their reasoning capabilities. This prompt-level intervention, \"Structured Reflective Chain-of-Thought,\" unlocks a deeper, more human-like iterative problem-solving process, leading to more robust and accurate solutions without any architectural changes or expensive fine-tuning. This advancement highlights the profound impact of sophisticated prompt engineering in leveraging latent reasoning abilities, pushing the boundaries of AI's capacity for self-correction and complex problem-solving, making AI systems more reliable and trustworthy for critical applications."
  }
}